"""
BeliefNormalizer - ìœ ì‚¬í•œ ë¯¿ìŒë“¤ì„ í†µí•©

ë¬¸ì œ: GPTê°€ ê°™ì€ ë¯¿ìŒì„ ë‹¤ë¥´ê²Œ í‘œí˜„ â†’ 889ê°œ ì¤‘ 887ê°œê°€ 1íšŒ ë“±ì¥
í•´ê²°: ìœ ì‚¬í•œ ë¯¿ìŒë“¤ì„ í•˜ë‚˜ì˜ "ì •ê·œí™”ëœ ë¯¿ìŒ"ìœ¼ë¡œ í†µí•©
"""

from openai import AsyncOpenAI
import os
import json
from typing import Dict, List, Tuple
from engines.utils.supabase_client import get_supabase

client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))

class BeliefNormalizer:
    """Normalize similar beliefs into canonical forms"""

    def __init__(self):
        self.supabase = get_supabase()

    async def normalize_all_beliefs(self, batch_size: int = 50) -> Dict:
        """
        Normalize all beliefs in belief_patterns table

        Args:
            batch_size: How many beliefs to process in one GPT call

        Returns:
            Dict with normalization stats
        """

        # 1. Get all beliefs
        print("\nğŸ“Š 1. ê¸°ì¡´ ë¯¿ìŒ ê°€ì ¸ì˜¤ê¸°...")
        beliefs = self.supabase.table('belief_patterns')\
            .select('id, belief, frequency, example_content_ids')\
            .execute().data

        print(f"âœ… ì´ {len(beliefs)}ê°œ ë¯¿ìŒ")

        # 2. Cluster similar beliefs
        print("\nğŸ” 2. ìœ ì‚¬ ë¯¿ìŒ í´ëŸ¬ìŠ¤í„°ë§...")
        clusters = await self._cluster_beliefs(beliefs, batch_size)

        print(f"âœ… {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„±")

        # 3. Update database
        print("\nğŸ’¾ 3. ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸...")
        stats = await self._update_clusters(clusters)

        return stats

    async def _cluster_beliefs(self, beliefs: List[Dict], batch_size: int) -> List[Dict]:
        """
        Cluster similar beliefs using GPT

        Returns:
            List of clusters, each with:
            - canonical_belief: ì •ê·œí™”ëœ ë¯¿ìŒ (ëŒ€í‘œ ë¬¸ì¥)
            - belief_ids: ì´ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ belief_pattern idë“¤
            - total_frequency: í•©ì‚° ë¹ˆë„
            - example_content_ids: í†µí•©ëœ ì˜ˆì‹œ content_idë“¤
        """

        clusters = []
        processed = set()

        # Process in batches
        for i in range(0, len(beliefs), batch_size):
            batch = [b for b in beliefs[i:i+batch_size] if b['id'] not in processed]

            if not batch:
                continue

            print(f"\r  ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì²˜ë¦¬ ì¤‘...", end='', flush=True)

            # Ask GPT to cluster this batch
            batch_clusters = await self._cluster_batch(batch)

            # Mark as processed
            for cluster in batch_clusters:
                for bid in cluster['belief_ids']:
                    processed.add(bid)

            clusters.extend(batch_clusters)

        print(f"\n  âœ… ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")

        return clusters

    async def _cluster_batch(self, batch: List[Dict]) -> List[Dict]:
        """
        Cluster a batch of beliefs
        """

        belief_list = "\n".join([
            f"{i+1}. [{b['id']}] {b['belief']} (ë¹ˆë„: {b['frequency']})"
            for i, b in enumerate(batch)
        ])

        prompt = f"""
ë‹¤ìŒì€ DC Gallery ìœ ì €ë“¤ì˜ "ì‹¬ì¸µ ë¯¿ìŒ"ë“¤ì…ë‹ˆë‹¤.
ì´ë“¤ ì¤‘ **ì˜ë¯¸ê°€ ìœ ì‚¬í•œ ê²ƒë“¤ì„ ê·¸ë£¹í™”**í•˜ê³ , ê° ê·¸ë£¹ì˜ **ëŒ€í‘œ ë¬¸ì¥**ì„ ë§Œë“œì„¸ìš”.

ë¯¿ìŒ ëª©ë¡:
{belief_list}

**ê·¸ë£¹í™” ê¸°ì¤€:**
1. í•µì‹¬ ë©”ì‹œì§€ê°€ ê°™ìœ¼ë©´ ê·¸ë£¹í™” (í‘œí˜„ì´ ë‹¬ë¼ë„)
   ì˜ˆ: "ë¯¼ì£¼ë‹¹ì€ ë…ì¬ë¥¼ í•œë‹¤" + "ë¯¼ì£¼ë‹¹ì€ ê³¼ê±° ë…ì¬ì •ê¶Œê³¼ ê°™ë‹¤" â†’ ê°™ì€ ê·¸ë£¹

2. êµ¬ì²´ì„± ìˆ˜ì¤€ì´ ë‹¤ë¥´ë©´ ê·¸ë£¹í™”
   ì˜ˆ: "ì¢ŒíŒŒëŠ” í­ë ¥ì ì´ë‹¤" + "ê·¹ì¢ŒëŠ” ë¬¼ë¦¬ì  ì¶©ëŒì„ ì„ í˜¸í•œë‹¤" â†’ ê°™ì€ ê·¸ë£¹

3. ì¸ê³¼ê´€ê³„ê°€ ê°™ìœ¼ë©´ ê·¸ë£¹í™”
   ì˜ˆ: "ì‘ì€ ì‚¬ì°°ì´ ë…ì¬ë¡œ ë°œì „" + "ì´ˆê¸° ê¶Œë ¥ë‚¨ìš©ì´ ì „ë©´ ì–µì••ìœ¼ë¡œ" â†’ ê°™ì€ ê·¸ë£¹

**ëŒ€í‘œ ë¬¸ì¥ ì‘ì„± ê¸°ì¤€:**
- ê°€ì¥ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ì„ íƒ
- ì •ì¹˜ì  ìš©ì–´ ìœ ì§€ (ë¯¼ì£¼ë‹¹, ì¢ŒíŒŒ ë“±)
- ì›ë¬¸ì˜ ë‰˜ì•™ìŠ¤ ë³´ì¡´

JSON í˜•ì‹:
{{
  "clusters": [
    {{
      "canonical_belief": "ì •ê·œí™”ëœ ë¯¿ìŒ (ëŒ€í‘œ ë¬¸ì¥)",
      "belief_ids": ["uuid1", "uuid2", ...],
      "reasoning": "ì™œ ì´ë“¤ì„ ë¬¶ì—ˆëŠ”ì§€ ê°„ë‹¨ ì„¤ëª…"
    }}
  ]
}}

âš ï¸ ì¤‘ìš”:
- í˜¼ìë§Œ ìˆëŠ” ë¯¿ìŒë„ í´ëŸ¬ìŠ¤í„° 1ê°œë¡œ ì¶œë ¥ (belief_idsì— ìê¸° idë§Œ)
- ëª¨ë“  ì…ë ¥ ë¯¿ìŒì´ ê²°ê³¼ì— ì •í™•íˆ 1ë²ˆì”© ë‚˜íƒ€ë‚˜ì•¼ í•¨
"""

        response = await client.chat.completions.create(
            model="gpt-4o",  # ë³µì¡í•œ ì‘ì—…ì´ë¯€ë¡œ gpt-4o ì‚¬ìš©
            messages=[
                {"role": "system", "content": "You are an expert in discourse analysis and text clustering."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
            timeout=120.0  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        result = json.loads(response.choices[0].message.content)

        # Calculate total frequency for each cluster
        clusters = []
        for cluster in result['clusters']:
            # Get beliefs in this cluster
            cluster_beliefs = [b for b in batch if b['id'] in cluster['belief_ids']]

            # Sum frequency
            total_freq = sum(b['frequency'] for b in cluster_beliefs)

            # Merge example_content_ids
            example_ids = []
            for b in cluster_beliefs:
                if b.get('example_content_ids'):
                    example_ids.extend(b['example_content_ids'])

            # Remove duplicates
            example_ids = list(set(example_ids))

            clusters.append({
                'canonical_belief': cluster['canonical_belief'],
                'belief_ids': cluster['belief_ids'],
                'total_frequency': total_freq,
                'example_content_ids': example_ids[:10]  # Keep max 10 examples
            })

        return clusters

    async def _update_clusters(self, clusters: List[Dict]) -> Dict:
        """
        Update belief_patterns table with cluster information

        Strategy:
        - Delete old individual beliefs
        - Insert new normalized beliefs
        """

        print(f"\n  ê¸°ì¡´ {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì •ê·œí™”ëœ ë¯¿ìŒìœ¼ë¡œ ë³€í™˜...")

        # 1. Delete all existing beliefs
        self.supabase.table('belief_patterns').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
        print(f"  âœ… ê¸°ì¡´ ë¯¿ìŒ ì‚­ì œ ì™„ë£Œ")

        # 2. Insert normalized beliefs
        normalized_beliefs = []

        for i, cluster in enumerate(clusters, 1):
            total_contents = len(self.supabase.table('layered_perceptions').select('id').execute().data)
            percentage = (cluster['total_frequency'] / total_contents * 100) if total_contents > 0 else 0

            normalized_beliefs.append({
                'belief': cluster['canonical_belief'],
                'frequency': cluster['total_frequency'],
                'percentage': round(percentage, 2),
                'example_content_ids': cluster['example_content_ids'],
                'cluster_id': None,  # Set to None instead of integer
                'cluster_name': f"cluster_{i}"
            })

        # Batch insert
        if normalized_beliefs:
            self.supabase.table('belief_patterns').insert(normalized_beliefs).execute()

        print(f"  âœ… {len(normalized_beliefs)}ê°œ ì •ê·œí™”ëœ ë¯¿ìŒ ì €ì¥ ì™„ë£Œ")

        # 3. Generate stats
        total_original = sum(len(c['belief_ids']) for c in clusters)
        reduction_rate = (1 - len(clusters) / total_original) * 100 if total_original > 0 else 0

        stats = {
            'original_count': total_original,
            'normalized_count': len(clusters),
            'reduction_rate': reduction_rate,
            'top_10': sorted(normalized_beliefs, key=lambda x: x['frequency'], reverse=True)[:10]
        }

        return stats

    async def show_normalization_preview(self, sample_size: int = 100):
        """
        Show a preview of what normalization would do (without saving)
        """

        beliefs = self.supabase.table('belief_patterns')\
            .select('id, belief, frequency')\
            .limit(sample_size)\
            .execute().data

        print(f"\nğŸ“Š ì •ê·œí™” ë¯¸ë¦¬ë³´ê¸° ({sample_size}ê°œ ìƒ˜í”Œ)")
        print("="*80)

        clusters = await self._cluster_beliefs(beliefs, batch_size=20)

        print(f"\nê²°ê³¼:")
        print(f"  ì›ë³¸: {len(beliefs)}ê°œ")
        print(f"  ì •ê·œí™” í›„: {len(clusters)}ê°œ")
        print(f"  ì¶•ì†Œìœ¨: {(1 - len(clusters)/len(beliefs)) * 100:.1f}%")

        print(f"\nìƒìœ„ 5ê°œ í´ëŸ¬ìŠ¤í„°:")
        top5 = sorted(clusters, key=lambda x: x['total_frequency'], reverse=True)[:5]
        for i, c in enumerate(top5, 1):
            print(f"\n{i}. {c['canonical_belief']}")
            print(f"   ë¹ˆë„: {c['total_frequency']}íšŒ")
            print(f"   í†µí•©ëœ ë¯¿ìŒ ìˆ˜: {len(c['belief_ids'])}ê°œ")
