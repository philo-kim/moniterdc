#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ëŸ¬ - ì¤‘ë³µ ë°©ì§€ ë¡œì§ í¬í•¨
ê²Œì‹œê¸€ ë²ˆí˜¸ì™€ ì‹œê°„ ê¸°ë°˜ìœ¼ë¡œ íš¨ìœ¨ì  ìˆ˜ì§‘
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from supabase import create_client
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv
import json
from datetime import datetime, timezone, timedelta
import logging
import re

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SmartCrawler:
    def __init__(self):
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_KEY')
        )
        self.openai_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.existing_posts = set()  # ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ë²ˆí˜¸ë“¤

    def load_existing_posts(self, gallery_id: str) -> set:
        """ì´ë¯¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ë²ˆí˜¸ë“¤ ë¡œë“œ"""
        try:
            result = self.supabase.table('logic_repository').select(
                'original_post_num'
            ).eq(
                'source_gallery', gallery_id
            ).not_.is_(
                'original_post_num', 'null'
            ).execute()

            existing = {str(row['original_post_num']) for row in result.data if row['original_post_num']}
            logger.info(f"ğŸ“‹ {gallery_id}: ê¸°ì¡´ ìˆ˜ì§‘ ê²Œì‹œê¸€ {len(existing)}ê°œ")
            return existing

        except Exception as e:
            logger.error(f"ê¸°ì¡´ ê²Œì‹œê¸€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return set()

    def get_last_crawl_time(self, gallery_id: str) -> datetime:
        """ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ í™•ì¸"""
        try:
            result = self.supabase.table('logic_repository').select(
                'created_at'
            ).eq(
                'source_gallery', gallery_id
            ).order(
                'created_at', desc=True
            ).limit(1).execute()

            if result.data:
                last_time = datetime.fromisoformat(result.data[0]['created_at'].replace('Z', '+00:00'))
                logger.info(f"ğŸ“… {gallery_id}: ë§ˆì§€ë§‰ ìˆ˜ì§‘ {last_time.strftime('%m-%d %H:%M')}")
                return last_time
            else:
                # ì²˜ìŒ ìˆ˜ì§‘í•˜ëŠ” ê²½ìš° 1ì‹œê°„ ì „ë¶€í„°
                return datetime.now(timezone.utc) - timedelta(hours=1)

        except Exception as e:
            logger.error(f"ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
            return datetime.now(timezone.utc) - timedelta(hours=1)

    def extract_post_number(self, post_elem) -> tuple:
        """ê²Œì‹œê¸€ì—ì„œ ë²ˆí˜¸ì™€ ì œëª© ì¶”ì¶œ"""
        try:
            # ê²Œì‹œê¸€ ë²ˆí˜¸
            num_elem = post_elem.select_one('.gall_num')
            if not num_elem or num_elem.text.strip() in ['ê³µì§€', 'AD', '']:
                return None, None

            try:
                post_num = int(num_elem.text.strip())
            except ValueError:
                return None, None

            # ì œëª©
            title_elem = post_elem.select_one('.gall_tit a')
            if not title_elem:
                return None, None

            title = title_elem.get_text(strip=True)
            if not title or len(title) < 10:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                return None, None

            return post_num, title

        except Exception as e:
            logger.error(f"ê²Œì‹œê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None, None

    async def get_post_content(self, session: aiohttp.ClientSession, post_url: str) -> str:
        """ê²Œì‹œê¸€ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (ì„ íƒì‚¬í•­)"""
        try:
            await asyncio.sleep(0.5)  # ìš”ì²­ ê°„ê²©
            async with session.get(post_url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    # DCê°¤ëŸ¬ë¦¬ ë³¸ë¬¸ ì¶”ì¶œ
                    content_elem = soup.select_one('.write_div, .writing_view_box')
                    if content_elem:
                        return content_elem.get_text(strip=True)[:1000]  # 1000ì ì œí•œ

        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        return ""

    async def analyze_with_gpt5(self, title: str, content: str = "") -> dict:
        """GPT-5ë¡œ ë…¼ë¦¬ ë¶„ì„"""
        try:
            text_to_analyze = f"ì œëª©: {title}\në‚´ìš©: {content[:500]}" if content else title

            # GPT-5 Responses API ì‚¬ìš©
            response = await self.openai_client.responses.create(
                input=f"""ì •ì¹˜ ê²Œì‹œê¸€ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{text_to_analyze}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "core_argument": "í•µì‹¬ ë…¼ì¦ ìš”ì•½",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "ai_classification": "ê³µê²©ì |ë°©ì–´ì |ì¤‘ë¦½ì ",
  "threat_level": 1-10,
  "effectiveness_score": 1-10,
  "category": "ì •ì¹˜|ê²½ì œ|ì‚¬íšŒ|ì™¸êµ|ê¸°íƒ€"
}}""",
                model="gpt-5",
                reasoning={
                    "effort": "low"
                },
                text={
                    "verbosity": "low"
                }
            )

            # Responses APIëŠ” ë‹¤ë¥¸ ì‘ë‹µ êµ¬ì¡°ë¥¼ ê°€ì§
            response_text = response.output_text if hasattr(response, 'output_text') else ""
            logger.info(f"GPT ì‘ë‹µ ê¸¸ì´: {len(response_text) if response_text else 0}, ë‚´ìš©: '{response_text}'")

            if not response_text or response_text.strip() == "":
                logger.warning(f"GPT-5ê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©: {title[:30]}...")
                return {
                    "core_argument": title[:100],
                    "keywords": title.split()[:3],
                    "ai_classification": "ë¶„ì„í•„ìš”",
                    "threat_level": 5,
                    "effectiveness_score": 5,
                    "category": "ê¸°íƒ€"
                }

            return json.loads(response_text)

        except Exception as e:
            logger.error(f"GPT ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "core_argument": title[:100],
                "keywords": title.split()[:3],
                "ai_classification": "ë¶„ì„í•„ìš”",
                "threat_level": 5,
                "effectiveness_score": 5,
                "category": "ê¸°íƒ€"
            }

    async def crawl_gallery(self, gallery_config: dict, max_posts: int = 20):
        """ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ (ì¤‘ë³µ ë°©ì§€ í¬í•¨)"""
        gallery_id = gallery_config['id']
        gallery_name = gallery_config['name']
        logic_type = gallery_config['logic_type']

        logger.info(f"ğŸ¯ {gallery_name} ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹œì‘")

        # 1. ê¸°ì¡´ ê²Œì‹œê¸€ ë²ˆí˜¸ ë¡œë“œ
        existing_posts = self.load_existing_posts(gallery_id)

        # 2. ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ í™•ì¸
        last_crawl = self.get_last_crawl_time(gallery_id)

        new_posts = []
        processed_count = 0

        async with aiohttp.ClientSession(headers={
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }) as session:

            # ìµœì‹  3í˜ì´ì§€ë§Œ ì²´í¬
            for page in range(1, 4):
                try:
                    if gallery_config.get('is_mgallery'):
                        url = f"https://gall.dcinside.com/mgallery/board/lists?id={gallery_id}&page={page}&exception_mode=recommend"
                    else:
                        url = f"https://gall.dcinside.com/board/lists?id={gallery_id}&page={page}&exception_mode=recommend"

                    await asyncio.sleep(0.5)  # ìš”ì²­ ê°„ê²©
                    async with session.get(url, timeout=10) as response:
                        if response.status != 200:
                            logger.warning(f"HTTP {response.status}: {url}")
                            continue

                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        posts = soup.select('tr.ub-content')

                        logger.info(f"ğŸ“„ {page}í˜ì´ì§€: {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")

                        for post in posts:
                            if processed_count >= max_posts:
                                break

                            post_num, title = self.extract_post_number(post)
                            if not post_num or not title:
                                continue

                            # ì¤‘ë³µ ì²´í¬
                            if str(post_num) in existing_posts:
                                logger.debug(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {post_num}")
                                continue

                            logger.info(f"ğŸ†• ìƒˆ ê²Œì‹œê¸€ {post_num}: {title[:50]}...")

                            # GPT ë¶„ì„
                            analysis = await self.analyze_with_gpt5(title)

                            # ê²Œì‹œê¸€ URL ìƒì„±
                            if gallery_config.get('is_mgallery'):
                                post_url = f"https://gall.dcinside.com/mgallery/board/view/?id={gallery_id}&no={post_num}&exception_mode=recommend"
                            else:
                                post_url = f"https://gall.dcinside.com/board/view/?id={gallery_id}&no={post_num}&exception_mode=recommend"

                            # DB ì €ì¥ ë°ì´í„° ì¤€ë¹„
                            data = {
                                'logic_type': logic_type,
                                'source_gallery': gallery_id,
                                'original_post_num': post_num,
                                'original_title': title,
                                'original_url': post_url,
                                'original_content': "",  # í•„ìš”ì‹œ ë³¸ë¬¸ë„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
                                'core_argument': analysis['core_argument'],
                                'keywords': analysis['keywords'][:5],
                                'ai_classification': analysis['ai_classification'],
                                'threat_level': min(10, max(1, analysis['threat_level'])),
                                'effectiveness_score': min(10, max(1, analysis['effectiveness_score'])),
                                'category': analysis.get('category', 'ê¸°íƒ€'),
                                'usage_count': 0,
                                'success_count': 0,
                                'is_active': True
                            }

                            # DBì— ì €ì¥
                            try:
                                result = self.supabase.table('logic_repository').insert(data).execute()
                                if result.data:
                                    processed_count += 1
                                    existing_posts.add(str(post_num))  # ì¤‘ë³µ ë°©ì§€ ì—…ë°ì´íŠ¸
                                    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {post_num}")

                            except Exception as e:
                                logger.error(f"ì €ì¥ ì‹¤íŒ¨ {post_num}: {e}")

                        if processed_count >= max_posts:
                            break

                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        logger.info(f"ğŸ‰ {gallery_name}: {processed_count}ê°œ ìƒˆ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return processed_count

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = SmartCrawler()

    # ê°¤ëŸ¬ë¦¬ ì„¤ì •
    galleries = [
        {
            'id': 'uspolitics',
            'name': 'ë¯¸êµ­ì •ì¹˜',
            'logic_type': 'attack',
            'is_mgallery': True
        },
        {
            'id': 'minjudang',
            'name': 'ë¯¼ì£¼ë‹¹',
            'logic_type': 'defense',
            'is_mgallery': True
        }
    ]

    total_collected = 0

    for gallery in galleries:
        try:
            count = await crawler.crawl_gallery(gallery, max_posts=10)
            total_collected += count
        except Exception as e:
            logger.error(f"{gallery['name']} ê°¤ëŸ¬ë¦¬ ì‹¤íŒ¨: {e}")

    logger.info(f"ğŸ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {total_collected}ê°œ ìƒˆ ê²Œì‹œê¸€")

    # ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
    try:
        result = crawler.supabase.table('logic_repository').select('*').order('created_at', desc=True).limit(5).execute()
        logger.info("ğŸ“Š ìµœê·¼ ìˆ˜ì§‘ ê²°ê³¼:")
        for logic in result.data:
            logger.info(f"  - {logic['source_gallery']}: {logic['original_title'][:50]}... (ìœ„í—˜ë„: {logic['threat_level']}/10)")
    except Exception as e:
        logger.error(f"ê²°ê³¼ í™•ì¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(main())