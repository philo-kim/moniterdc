# 세계관 분석 시스템 V4 - 구현 로드맵

## 목표 재확인

**핵심 목적**: DC Gallery 유저들이 구성한 세계관을 이해하기
- 표면 주장이 아닌, 숨어있는 사고방식과 무의식적 믿음 체계 파악
- 개별 의견이 아닌, 반복되는 패턴과 구조 발견
- 통계적으로 의미있는 분석

## 전체 구조 (4개 Layer)

```
Layer 1: 데이터 수집
   ↓
Layer 2: 3층 구조 분석 (표면/암묵/심층)
   ↓
Layer 3: 패턴 탐지 (반복되는 믿음 찾기)
   ↓
Layer 4: 세계관 구성 (믿음들의 연결 구조)
```

---

## Phase 0: 사전 준비 및 검증 ⏱️ 30분

### 목표
현재 시스템 상태 파악 및 데이터 품질 확인

### 체크리스트

#### 0.1 현재 데이터 상태 확인
- [ ] DB에 있는 contents 개수 확인
- [ ] 본문이 있는 contents 개수 확인 (body != '')
- [ ] 샘플 10개 글 읽어보고 품질 확인
  - 본문 길이가 충분한가?
  - 정치 담론 분석에 적합한가?
  - 중복이나 의미없는 글은 없는가?

#### 0.2 기존 테이블 확인
- [ ] contents 테이블 스키마 확인
- [ ] perceptions 테이블이 있다면 데이터 확인
- [ ] worldviews 테이블 스키마 확인

#### 0.3 API 키 및 환경 확인
- [ ] OPENAI_API_KEY 설정 확인
- [ ] Supabase 연결 테스트
- [ ] GPT-4o-mini 호출 테스트 (간단한 요청)

### 완료 조건
- 최소 50개 이상의 본문 있는 contents 확인
- Supabase 및 OpenAI API 정상 작동 확인
- 샘플 데이터의 품질이 분석에 적합함을 확인

### 위험 요소
- 데이터가 부족하면 Phase 1로 먼저 이동
- API 오류가 있으면 해결 후 진행

---

## Phase 1: 데이터 수집 (필요시) ⏱️ 1-2시간

### 목표
최소 200개 이상의 양질의 글 확보

### 체크리스트

#### 1.1 수집 전략 수립
- [ ] 어느 갤러리에서 수집할지 결정
  - uspolitics (미국정치 마갤) - 현재 사용 중
  - 추가 갤러리 필요시 검토
- [ ] 수집 기간 설정 (최근 1-2개월)
- [ ] concept_only vs 전체 글 결정

#### 1.2 ContentCollector 검증
- [ ] ContentCollector.collect() API 확인
- [ ] 테스트: 10개만 수집해서 결과 확인
- [ ] 본문 추출이 제대로 되는지 확인

#### 1.3 점진적 수집
- [ ] 1차: 50개 수집 → 품질 확인
- [ ] 2차: 100개 수집 → 품질 확인
- [ ] 3차: 200개까지 수집
- [ ] 각 단계마다 중복 제거 확인

### 완료 조건
- 최소 200개의 본문 있는 contents
- 중복 없음
- 정치 담론 분석에 적합한 내용

### 위험 요소
- 크롤링 차단: 속도 조절, 시간 간격 두기
- 본문 없는 글이 많음: concept_only 옵션 조정
- 중복 발생: content URL 기반 중복 체크

### 롤백 계획
- 수집 실패시: 기존 81개로 Phase 2 진행 가능

---

## Phase 2: DB 스키마 준비 ⏱️ 30분

### 목표
새로운 분석 결과를 저장할 테이블 생성

### 체크리스트

#### 2.1 마이그레이션 파일 검토
- [ ] 201_create_layered_perceptions.sql 검토
  - 컬럼 타입이 적절한가?
  - 인덱스가 필요한 곳에 있는가?
- [ ] 202_create_belief_patterns.sql 검토
  - JSONB 필드 구조가 명확한가?
  - 통계 분석에 필요한 컬럼이 있는가?

#### 2.2 기존 테이블과의 관계 확인
- [ ] layered_perceptions.content_id → contents.id (FK)
- [ ] belief_patterns는 독립 테이블
- [ ] worldviews 테이블에 새 컬럼 추가 필요 여부 확인

#### 2.3 마이그레이션 적용
- [ ] Supabase Dashboard → SQL Editor 접속
- [ ] 201번 마이그레이션 실행
- [ ] 테이블 생성 확인 (SELECT * FROM layered_perceptions LIMIT 1)
- [ ] 202번 마이그레이션 실행
- [ ] 테이블 생성 확인 (SELECT * FROM belief_patterns LIMIT 1)

### 완료 조건
- layered_perceptions 테이블 생성 완료
- belief_patterns 테이블 생성 완료
- FK 제약조건 정상 작동

### 위험 요소
- SQL 구문 오류: 마이그레이션 파일 문법 검증
- FK 제약 실패: contents 테이블 먼저 확인
- 권한 문제: Supabase service key 확인

### 롤백 계획
```sql
DROP TABLE IF EXISTS layered_perceptions CASCADE;
DROP TABLE IF EXISTS belief_patterns CASCADE;
```

---

## Phase 3: Layer 2 구현 - 3층 구조 분석 ⏱️ 2-3시간

### 목표
각 글을 표면/암묵/심층 3개 층위로 분석

### 체크리스트

#### 3.1 LayeredPerceptionExtractor 코드 검증
- [ ] GPT 프롬프트 품질 확인
  - 3개 층위 구분이 명확한가?
  - 예시가 충분히 제시되었는가?
  - JSON 스키마가 정확한가?
- [ ] 단일 글로 테스트
  - 1개 글 선택해서 extract() 실행
  - 결과 JSON 구조 확인
  - 각 층위의 내용이 의미있는가?

#### 3.2 에러 처리 강화
- [ ] GPT API 오류 처리 (timeout, rate limit)
- [ ] JSON 파싱 오류 처리
- [ ] DB 저장 실패 처리
- [ ] 재시도 로직 (최대 3회)

#### 3.3 점진적 실행
- [ ] 1단계: 10개 글 분석
  - 결과 확인: DB에 저장되었는가?
  - 품질 확인: 층위 구분이 명확한가?
  - 평균 처리 시간 측정
- [ ] 2단계: 50개 글 분석
  - 누적 비용 확인 (OpenAI API)
  - 오류율 확인 (90% 이상 성공)
- [ ] 3단계: 전체 분석
  - 배치 크기 설정 (20개씩)
  - 진행상황 표시
  - 중간 저장 (실패시 재시작 가능)

#### 3.4 결과 검증
- [ ] layered_perceptions 테이블 확인
  - 총 레코드 수 = contents 수
  - NULL 값이 너무 많지 않은가?
- [ ] 샘플 10개 수동 검토
  - deep_beliefs가 실제로 "무의식적 믿음"인가?
  - implicit_assumptions가 "암묵적 전제"인가?
  - explicit_claims가 정확히 추출되었는가?

### 완료 조건
- 전체 contents의 90% 이상 분석 완료
- layered_perceptions 테이블 채워짐
- 수동 검토 결과 품질 합격

### 위험 요소
- GPT API 비용 과다: 배치 크기 조정, gpt-4o-mini 사용
- Rate limit 초과: 요청 간격 조절 (sleep)
- 품질 저하: 프롬프트 개선 및 재실행

### 롤백 계획
- 분석 실패시: layered_perceptions 테이블 비우고 재시작
```sql
DELETE FROM layered_perceptions WHERE created_at > '2025-01-03';
```

---

## Phase 4: Layer 3 구현 - 패턴 탐지 ⏱️ 1-2시간

### 목표
반복되는 deep_beliefs 찾고 통계 분석

### 체크리스트

#### 4.1 BeliefPatternDetector 설계 검증
- [ ] 빈도 분석 로직 확인
  - deep_beliefs를 어떻게 그룹화할 것인가?
  - 완전 일치 vs 유사도 기반
- [ ] 유의미성 기준 설정
  - 30% 이상: 핵심 믿음
  - 10-30%: 부분 믿음
  - 10% 미만: 개별 의견

#### 4.2 단계별 구현

**4.2.1 빈도 분석**
- [ ] 모든 layered_perceptions에서 deep_beliefs 추출
- [ ] 믿음별 출현 횟수 계산
- [ ] 백분율 계산
- [ ] 상위 20개 믿음 확인

**4.2.2 Co-occurrence 분석**
- [ ] 같은 글에 함께 나타나는 믿음 쌍 찾기
- [ ] 빈도 계산
- [ ] 상관관계 높은 쌍 식별

**4.2.3 클러스터링**
- [ ] 유사한 믿음들 그룹화
  - 예: "권력은 부패한다" + "권력자는 감시를 악용한다"
- [ ] 클러스터 이름 생성 (GPT 활용)
  - 예: "권력 비관론"

#### 4.3 belief_patterns 테이블 채우기
- [ ] 각 믿음마다 레코드 생성
- [ ] frequency, percentage 저장
- [ ] co_occurring_beliefs 저장
- [ ] cluster_id 할당

#### 4.4 결과 검증
- [ ] 상위 10개 믿음 수동 확인
  - 실제로 반복되는가?
  - 의미있는 패턴인가?
- [ ] 통계적 유의미성 확인
  - 핵심 믿음이 5-10개 정도?
  - 분포가 합리적인가?

### 완료 조건
- belief_patterns 테이블 채워짐 (20-50개 레코드)
- 핵심 믿음 5-10개 식별
- Co-occurrence 관계 파악됨

### 위험 요소
- 믿음 표현이 너무 다양함: 정규화/표준화 필요
- 유의미한 패턴이 없음: 데이터 양 부족 또는 분석 방법 재검토
- 클러스터링 실패: 수동 그룹화로 대체

### 롤백 계획
```sql
DELETE FROM belief_patterns;
```

---

## Phase 5: Layer 4 구현 - 세계관 구성 ⏱️ 1-2시간

### 목표
패턴들을 연결하여 전체 세계관 구조 구성

### 체크리스트

#### 5.1 WorldviewSynthesizer 설계 검증
- [ ] 입력: belief_patterns + layered_perceptions
- [ ] 출력: 통합된 세계관 구조
- [ ] 연결 방식 설계
  - 수직적: 믿음 → 사고 → 주장
  - 수평적: 믿음들 간 관계

#### 5.2 수직적 구조 분석
- [ ] 각 핵심 믿음에 대해:
  - 어떤 암묵적 사고를 생성하는가?
  - 어떤 표면 주장으로 나타나는가?
- [ ] 예시 글 찾기 (각 믿음마다 3-5개)
- [ ] 연결 고리의 논리성 검증

#### 5.3 수평적 구조 분석
- [ ] 믿음들 간의 관계 파악
  - A가 B를 정당화하는가?
  - A와 B가 함께 C를 생성하는가?
- [ ] 중심 믿음 vs 파생 믿음 구분
- [ ] 순환 구조 찾기

#### 5.4 전체 서사 생성
- [ ] GPT-4o로 종합 분석 요청
  - 입력: 모든 패턴 데이터
  - 출력: 세계관의 작동 원리 설명
- [ ] 서사 검증
  - 실제 데이터와 일치하는가?
  - 과도한 해석은 없는가?

#### 5.5 worldviews 테이블 저장
- [ ] 새 worldview 레코드 생성
- [ ] core_beliefs 저장
- [ ] vertical_structure 저장
- [ ] horizontal_structure 저장
- [ ] overall_structure 저장
- [ ] statistical_confidence 저장

### 완료 조건
- worldviews 테이블에 1개 레코드 생성
- 수직/수평 구조 명확히 문서화됨
- 통계적 신뢰도 정보 포함

### 위험 요소
- 구조가 너무 복잡함: 핵심만 추출
- 연결이 약함: 데이터 부족 또는 방법론 재검토
- GPT 해석이 과도함: 통계 기반으로 검증

### 롤백 계획
- worldviews 레코드 삭제 후 재생성

---

## Phase 6: 검증 및 개선 ⏱️ 1-2시간

### 목표
전체 결과의 품질과 의미 검증

### 체크리스트

#### 6.1 데이터 품질 검증
- [ ] 각 Phase의 데이터 일관성 확인
  - Layer 2: layered_perceptions 품질
  - Layer 3: belief_patterns 유의미성
  - Layer 4: worldview 구조 타당성

#### 6.2 통계적 검증
- [ ] 핵심 믿음의 빈도가 충분한가? (30% 이상)
- [ ] 샘플 편향이 없는가?
- [ ] 과적합(overfitting)은 없는가?

#### 6.3 의미 검증
- [ ] 실제 10개 글을 읽고 세계관 구조와 대조
  - 구조가 실제 글들을 설명하는가?
  - 놓친 중요한 패턴은 없는가?
  - 과도한 일반화는 없는가?

#### 6.4 목적 달성 확인
- [ ] "숨어있는 세계관"을 파악했는가?
  - 표면 주장만이 아닌 심층 믿음 발견
  - 무의식적 전제들 식별
- [ ] "이해하기 쉽게" 정리했는가?
  - 3층 구조가 명확한가?
  - 연결 고리가 이해되는가?

#### 6.5 개선 사항 식별
- [ ] 품질 문제 발견시 개선 계획 수립
  - 프롬프트 수정 필요?
  - 데이터 추가 수집 필요?
  - 분석 방법 조정 필요?

### 완료 조건
- 전체 시스템의 품질이 목적에 부합
- 발견된 세계관이 실제 데이터를 설명
- 개선 사항 문서화

---

## Phase 7: Dashboard 업데이트 (선택) ⏱️ 2-3시간

### 목표
분석 결과를 시각적으로 표시

### 체크리스트

#### 7.1 API 엔드포인트 추가
- [ ] GET /api/beliefs - 핵심 믿음 목록
- [ ] GET /api/beliefs/[id] - 믿음 상세 (수직 구조)
- [ ] GET /api/worldview/structure - 전체 구조
- [ ] GET /api/worldview/statistics - 통계

#### 7.2 UI 컴포넌트
- [ ] 메인 페이지: 핵심 믿음 카드
- [ ] 상세 페이지: 층위별 연결 시각화
- [ ] 구조 페이지: 믿음 간 관계 그래프

#### 7.3 시각화
- [ ] 빈도 차트 (막대 그래프)
- [ ] 연결 그래프 (네트워크 다이어그램)
- [ ] 층위 흐름도 (믿음 → 사고 → 주장)

### 완료 조건
- Dashboard에서 분석 결과 확인 가능
- 시각화가 이해하기 쉬움

---

## 전체 타임라인

### 최소 버전 (Phase 0-5)
- **총 소요 시간**: 6-9시간
- **산출물**:
  - 200개 글의 3층 구조 분석
  - 핵심 믿음 5-10개 식별
  - 세계관 구조 1개 생성

### 완전 버전 (Phase 0-7)
- **총 소요 시간**: 8-12시간
- **산출물**:
  - 위 + Dashboard UI

---

## 안전장치 (각 Phase마다 적용)

### 1. 점진적 진행
- 작은 단위로 테스트 → 확인 → 전체 실행
- 10개 → 50개 → 전체

### 2. 중간 저장
- 각 Phase 완료시 결과 DB 저장
- 실패시 롤백 가능

### 3. 품질 검증
- 각 Phase 완료시 샘플 수동 확인
- 기준 미달시 다음 Phase 진행 안 함

### 4. 비용 모니터링
- OpenAI API 사용량 추적
- 예상 비용 초과시 경고

### 5. 에러 처리
- 모든 외부 호출에 try-catch
- 재시도 로직 (최대 3회)
- 로그 기록

---

## 성공 기준

### 필수 조건
- [ ] 최소 200개 글 분석 완료
- [ ] 핵심 믿음 5개 이상 식별 (빈도 30% 이상)
- [ ] 세계관 구조 생성 완료

### 품질 기준
- [ ] 3층 구조가 명확히 구분됨
- [ ] 믿음 → 사고 → 주장의 연결이 논리적
- [ ] 실제 글 10개가 세계관 구조로 설명됨

### 목적 달성
- [ ] "숨어있는" 믿음 발견 (표면 주장만이 아님)
- [ ] "이해하기 쉽게" 구조화됨
- [ ] 통계적 근거가 명확함

---

## 다음 단계

이 로드맵 검토 후:

1. **Phase 0부터 시작** - 현재 상태 확인
2. **각 Phase 완료시 확인** - 품질 검증 후 다음 진행
3. **문제 발생시** - 해당 Phase 롤백 후 재시도

시작할까요?
