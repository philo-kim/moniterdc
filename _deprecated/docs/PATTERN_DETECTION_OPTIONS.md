# íŒ¨í„´ íƒì§€ ë°©ë²•ë¡  ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼

## í˜„ì¬ ìƒí™©
- 893ê°œ ë¯¿ìŒ, 889ê°œ ê³ ìœ  â†’ ê±°ì˜ ë°˜ë³µ ì—†ìŒ
- ë¬¸ì œ: ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ í‘œí˜„ì´ ë‹¬ë¼ì„œ íŒ¨í„´ íƒì§€ ì‹¤íŒ¨
- ì˜ˆ: "ë¯¼ì£¼ë‹¹ì€ ë…ì¬" vs "ì¢ŒíŒŒëŠ” ì–µì••" (ê°™ì€ ì˜ë¯¸, ë‹¤ë¥¸ í‘œí˜„)

---

## ğŸ¯ Level 1: ê¸°ë³¸ ë°©ì‹ (ë‹¨ìˆœí•˜ì§€ë§Œ ì œí•œì )

### 1.1 í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
```python
frames = {
    "ë¯¼ì£¼ë‹¹_ë¶€ì •": ["ë¯¼ì£¼ë‹¹", "ì¢ŒíŒŒ", "ë¶€ì •", "ì¡°ì‘"],
    "ë…ì¬_ìš°ë ¤": ["ë…ì¬", "ì–µì••", "ì‚¬ì°°", "ê°ì‹œ"],
    "ê³„ì—„_ì •ë‹¹": ["ê³„ì—„", "êµ°", "ì•ˆë³´", "ì§ˆì„œ"]
}
```
**ì¥ì **: ë¹ ë¦„, ê°„ë‹¨
**ë‹¨ì **: í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ëˆ„ë½, ë§¥ë½ ë¬´ì‹œ

### 1.2 ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
```python
patterns = [
    r"(ë¯¼ì£¼ë‹¹|ì¢ŒíŒŒ).*(ë…ì¬|ì–µì••|ì‚¬ì°°)",
    r"ê³„ì—„.*(í•„ìš”|ì •ë‹¹|ì•ˆì „)"
]
```
**ì¥ì **: êµ¬ì¡°ì  íŒ¨í„´ ê°ì§€
**ë‹¨ì **: ë³µì¡í•œ ì˜ë¯¸ ëª»ì¡ìŒ

---

## ğŸ”¬ Level 2: í†µê³„/ML ê¸°ë°˜ (ì¤‘ê¸‰)

### 2.1 TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(beliefs)
similarity = cosine_similarity(tfidf_matrix)
```
**ì¥ì **: ë¹ ë¦„, ì „í†µì ìœ¼ë¡œ ê²€ì¦ë¨
**ë‹¨ì **: ë‹¨ì–´ ìˆ˜ì¤€, ì˜ë¯¸ ì´í•´ ë¶€ì¡±

### 2.2 LDA (Latent Dirichlet Allocation) í† í”½ ëª¨ë¸ë§
```python
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=10)
topics = lda.fit_transform(tfidf_matrix)
```
**ì¥ì **: ìˆ¨ì€ ì£¼ì œ ìë™ ë°œê²¬
**ë‹¨ì **: ì§§ì€ í…ìŠ¤íŠ¸ì— ì•½í•¨, í•´ì„ ì–´ë ¤ì›€

### 2.3 K-means í´ëŸ¬ìŠ¤í„°ë§ (Embedding ê¸°ë°˜)
```python
from sklearn.cluster import KMeans
import openai

embeddings = [openai.Embedding.create(input=b)['data'][0]['embedding']
              for b in beliefs]
kmeans = KMeans(n_clusters=20)
clusters = kmeans.fit_predict(embeddings)
```
**ì¥ì **: ì˜ë¯¸ ê¸°ë°˜, í™•ì¥ì„± ì¢‹ìŒ
**ë‹¨ì **: K ì‚¬ì „ ê²°ì •, í´ëŸ¬ìŠ¤í„° ì˜ë¯¸ ë¶ˆëª…í™•

### 2.4 DBSCAN (ë°€ë„ ê¸°ë°˜)
```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.3, min_samples=5)
clusters = dbscan.fit_predict(embeddings)
```
**ì¥ì **: K ë¶ˆí•„ìš”, ë…¸ì´ì¦ˆ íƒì§€
**ë‹¨ì **: íŒŒë¼ë¯¸í„° ë¯¼ê°, í¬ì†Œ ë°ì´í„°ì— ì•½í•¨

### 2.5 Hierarchical Clustering (ê³„ì¸µì )
```python
from scipy.cluster.hierarchy import dendrogram, linkage

linkage_matrix = linkage(embeddings, method='ward')
# ë´ë“œë¡œê·¸ë¨ìœ¼ë¡œ ì‹œê°í™” â†’ ìˆ˜ë™ìœ¼ë¡œ cut
```
**ì¥ì **: ê³„ì¸µ êµ¬ì¡° ë³´ì„, ìœ ì—°í•œ cut
**ë‹¨ì **: ëŠë¦¼ (O(nÂ²)), ëŒ€ëŸ‰ ë°ì´í„° ë¶€ì í•©

---

## ğŸ§  Level 3: ë”¥ëŸ¬ë‹/ì„ë² ë”© ê³ ê¸‰ (ìƒê¸‰)

### 3.1 Sentence-BERT (SBERT) ìœ ì‚¬ë„ + Community Detection
```python
from sentence_transformers import SentenceTransformer
import networkx as nx

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(beliefs)

# ìœ ì‚¬ë„ ê·¸ë˜í”„ â†’ ì»¤ë®¤ë‹ˆí‹° íƒì§€
G = nx.Graph()
for i, j in similar_pairs:  # cosine > 0.7
    G.add_edge(i, j)

communities = nx.community.louvain_communities(G)
```
**ì¥ì **: ì˜ë¯¸ ìœ ì‚¬ë„ ì •í™•, ìë™ ê·¸ë£¹í™”
**ë‹¨ì **: ì„ê³„ê°’ ì„¤ì • í•„ìš”, í•´ì„ì„±

### 3.2 UMAP + HDBSCAN (ì°¨ì›ì¶•ì†Œ + í´ëŸ¬ìŠ¤í„°ë§)
```python
import umap
import hdbscan

# ê³ ì°¨ì› â†’ ì €ì°¨ì›
reducer = umap.UMAP(n_components=5)
reduced = reducer.fit_transform(embeddings)

# ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
clusterer = hdbscan.HDBSCAN(min_cluster_size=10)
clusters = clusterer.fit_predict(reduced)
```
**ì¥ì **: ë³µì¡í•œ êµ¬ì¡° íƒì§€, ìë™ í´ëŸ¬ìŠ¤í„° ìˆ˜
**ë‹¨ì **: ë¸”ë™ë°•ìŠ¤, ì¬í˜„ì„± ë‚®ìŒ

### 3.3 BERTopic (BERT + c-TF-IDF)
```python
from bertopic import BERTopic

topic_model = BERTopic(language="korean")
topics, probs = topic_model.fit_transform(beliefs)

# í† í”½ë³„ ëŒ€í‘œ ë‹¨ì–´ ìë™ ì¶”ì¶œ
topic_model.get_topic_info()
```
**ì¥ì **: ìë™í™”, í•´ì„ ê°€ëŠ¥í•œ í† í”½
**ë‹¨ì **: í•œêµ­ì–´ ì§€ì› ì œí•œ, ì§§ì€ ë¬¸ì¥ ì•½í•¨

---

## ğŸ¤– Level 4: LLM ê¸°ë°˜ (ìµœê³ ê¸‰)

### 4.1 Few-shot Classification (GPT)
```python
prompt = f"""
ë‹¤ìŒ ë¯¿ìŒë“¤ì„ 10ê°€ì§€ í”„ë ˆì„ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

ì˜ˆì‹œ:
- "ë¯¼ì£¼ë‹¹ì€ ë…ì¬" â†’ í”„ë ˆì„: ë¯¼ì£¼ë‹¹_ë…ì¬ë¡ 
- "ì¢ŒíŒŒëŠ” ì–µì••" â†’ í”„ë ˆì„: ë¯¼ì£¼ë‹¹_ë…ì¬ë¡ 

ë¶„ë¥˜ ëŒ€ìƒ:
{beliefs[:100]}

ì¶œë ¥: JSON {{"belief": "...", "frame": "..."}}
"""
```
**ì¥ì **: ë§¥ë½ ì´í•´, ìœ ì—°í•¨
**ë‹¨ì **: ë¹„ìš©, í† í° ì œí•œ, ì¼ê´€ì„± ë¬¸ì œ

### 4.2 Chain-of-Thought Clustering
```python
# Step 1: GPTê°€ ê° ë¯¿ìŒì˜ í•µì‹¬ ê°œë… ì¶”ì¶œ
concepts = gpt.extract_core_concepts(beliefs)

# Step 2: ê°œë… ê¸°ë°˜ ê·¸ë£¹í™”
groups = gpt.group_by_concepts(concepts)

# Step 3: ê° ê·¸ë£¹ì˜ ê³µí†µ íŒ¨í„´ ì„œìˆ 
patterns = gpt.describe_patterns(groups)
```
**ì¥ì **: ì¸ê°„ ìˆ˜ì¤€ ì´í•´, ì„¤ëª… ê°€ëŠ¥
**ë‹¨ì **: ëŠë¦¼, ë¹„ìŒˆ, ë°°ì¹˜ ì²˜ë¦¬ ë³µì¡

### 4.3 Embedding + LLM í•˜ì´ë¸Œë¦¬ë“œ (ì¶”ì²œ!)
```python
# Step 1: Embeddingìœ¼ë¡œ í›„ë³´ ê·¸ë£¹ ìƒì„± (ë¹ ë¦„)
embeddings = openai.Embedding.create(...)
rough_clusters = cluster(embeddings, threshold=0.75)

# Step 2: ê° í´ëŸ¬ìŠ¤í„°ë¥¼ GPTê°€ ê²€í†  ë° ì •ì œ
for cluster in rough_clusters:
    refined = gpt.refine_cluster(cluster)
    pattern = gpt.extract_common_pattern(refined)
```
**ì¥ì **: ì†ë„ + ì •í™•ë„ ê· í˜•
**ë‹¨ì **: 2ë‹¨ê³„ ì²˜ë¦¬

### 4.4 Recursive Summarization
```python
# 889ê°œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìš”ì•½
# Level 1: 889 â†’ 89 (10ê°œì”© ë¬¶ì–´ íŒ¨í„´ ì¶”ì¶œ)
# Level 2: 89 â†’ 9 (10ê°œì”© ë¬¶ì–´ ìƒìœ„ íŒ¨í„´)
# Level 3: 9 â†’ ìµœì¢… í”„ë ˆì„

def recursive_pattern(beliefs, level=0):
    if len(beliefs) <= 10:
        return gpt.extract_final_patterns(beliefs)

    # 10ê°œì”© ë¬¶ì–´ì„œ ìš”ì•½
    summaries = []
    for batch in chunks(beliefs, 10):
        summary = gpt.extract_patterns(batch)
        summaries.append(summary)

    return recursive_pattern(summaries, level+1)
```
**ì¥ì **: ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬, ê³„ì¸µì  íŒ¨í„´
**ë‹¨ì **: ì •ë³´ ì†ì‹¤ ê°€ëŠ¥, ë³µì¡

### 4.5 Graph RAG (ê´€ê³„ ê¸°ë°˜)
```python
# ë¯¿ìŒë“¤ì„ ë…¸ë“œë¡œ, ì˜ë¯¸ì  ê´€ê³„ë¥¼ ì—£ì§€ë¡œ
graph = build_belief_graph(beliefs)

# PageRankë¡œ ì¤‘ì‹¬ ë¯¿ìŒ ì°¾ê¸°
central_beliefs = pagerank(graph, top_k=20)

# ì¤‘ì‹¬ ë¯¿ìŒ ì£¼ë³€ì˜ í´ëŸ¬ìŠ¤í„° í˜•ì„±
for central in central_beliefs:
    cluster = get_neighborhood(graph, central, radius=2)
    pattern = gpt.describe_cluster(cluster)
```
**ì¥ì **: ê´€ê³„ êµ¬ì¡° ë°˜ì˜, ì¤‘ìš”ë„ ìë™
**ë‹¨ì **: ê·¸ë˜í”„ êµ¬ì„± ë³µì¡

---

## ğŸš€ Level 5: ìµœì‹  ì—°êµ¬ ê¸°ë²•

### 5.1 Contrastive Learning (ëŒ€ì¡° í•™ìŠµ)
```python
# ìœ ì‚¬í•œ ë¯¿ìŒì€ ê°€ê¹ê²Œ, ë‹¤ë¥¸ ë¯¿ìŒì€ ë©€ê²Œ
from sentence_transformers import losses

model.fit(
    train_objectives=[(dataloader, losses.ContrastiveLoss())]
)
```
**ì¥ì **: ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ì •í™•ë„ ê·¹ëŒ€í™”
**ë‹¨ì **: í•™ìŠµ ë°ì´í„° í•„ìš”, ì‹œê°„

### 5.2 Prompt-based Clustering (GPT-4 + Self-consistency)
```python
# ê°™ì€ ìš”ì²­ì„ 5ë²ˆ â†’ íˆ¬í‘œë¡œ ìµœì¢… ê²°ì •
results = []
for _ in range(5):
    result = gpt4.cluster(beliefs, temperature=0.7)
    results.append(result)

# 5ê°œ ê²°ê³¼ì˜ êµì§‘í•© â†’ ì•ˆì •ì ì¸ í´ëŸ¬ìŠ¤í„°
stable_clusters = voting(results)
```
**ì¥ì **: ë†’ì€ ì‹ ë¢°ë„
**ë‹¨ì **: 5ë°° ë¹„ìš©

### 5.3 Active Learning (ì¸ê°„ í”¼ë“œë°±)
```python
# ë¶ˆí™•ì‹¤í•œ ê²½ê³„ë§Œ ì‚¬ëŒì´ íŒë‹¨
uncertain_pairs = find_boundary_cases(embeddings)

for pair in uncertain_pairs:
    human_label = ask_user(pair)  # "ê°™ì€ í”„ë ˆì„?" Y/N
    model.update(human_label)
```
**ì¥ì **: ìµœì†Œ ë…¸ë ¥ìœ¼ë¡œ ìµœëŒ€ ì •í™•ë„
**ë‹¨ì **: ì¸ê°„ ê°œì… í•„ìš”

---

## ğŸ’¡ ìš°ë¦¬ ë°ì´í„°ì— ì í•©í•œ ë°©ì‹ ì¶”ì²œ ìˆœìœ„

### ğŸ¥‡ 1ìˆœìœ„: Embedding + Threshold Clustering + GPT ìš”ì•½
```python
# 1. OpenAI Embeddingìœ¼ë¡œ ë²¡í„°í™”
# 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ 0.8 ì´ìƒ â†’ ê°™ì€ ê·¸ë£¹
# 3. ê° ê·¸ë£¹ì„ GPTê°€ "ê³µí†µ íŒ¨í„´" ì„œìˆ 
# 4. 10ê°œ ë¯¸ë§Œ ê·¸ë£¹ì€ "ê°œë³„ ì˜ê²¬"ìœ¼ë¡œ ë¶„ë¥˜
```
**ì´ìœ **:
- âœ… 889ê°œ ì²˜ë¦¬ ê°€ëŠ¥ (ë¹ ë¦„)
- âœ… ì˜ë¯¸ ê¸°ë°˜ (ì •í™•í•¨)
- âœ… í•´ì„ ê°€ëŠ¥ (GPT ìš”ì•½)
- âœ… ë¹„ìš© ì ì • (~$0.50)

### ğŸ¥ˆ 2ìˆœìœ„: BERTopic (í•œêµ­ì–´ ëª¨ë¸)
```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
topic_model = BERTopic(embedding_model=embedding_model)
topics, probs = topic_model.fit_transform(beliefs)
```
**ì´ìœ **:
- âœ… ì™„ì „ ìë™í™”
- âœ… í† í”½ í•´ì„ ì‰¬ì›€
- âš ï¸ ì§§ì€ ë¬¸ì¥ì— ì•½í•  ìˆ˜ ìˆìŒ

### ğŸ¥‰ 3ìˆœìœ„: Recursive GPT Summarization
```python
# 889 â†’ 89 â†’ 9 â†’ ìµœì¢…
def recursive_cluster(beliefs, batch_size=10):
    if len(beliefs) <= 10:
        return gpt.final_patterns(beliefs)

    patterns = []
    for batch in chunks(beliefs, batch_size):
        pattern = gpt.extract_pattern(batch)
        patterns.append(pattern)

    return recursive_cluster(patterns, batch_size)
```
**ì´ìœ **:
- âœ… ëŒ€ëŸ‰ ë°ì´í„° ê°€ëŠ¥
- âœ… ê³„ì¸µì  ì´í•´
- âš ï¸ ì •ë³´ ì†ì‹¤ ìœ„í—˜

---

## ğŸ¯ ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥í•œ ì‹¤ìš©ì  ë°©ë²•

### Option A: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… (30ë¶„)
```python
# í‚¤ì›Œë“œ ê¸°ë°˜ ì´ˆë²Œ ë¶„ë¥˜ â†’ GPT ì •ì œ
frames = keyword_classify(beliefs)
for frame in frames:
    pattern = gpt.summarize(frame['beliefs'][:50])
    save_pattern(frame['name'], pattern)
```

### Option B: ì •í™•ë„ ìš°ì„  (2ì‹œê°„)
```python
# Embedding + Community Detection
embeddings = openai_embed(beliefs)
graph = build_similarity_graph(embeddings, threshold=0.8)
communities = detect_communities(graph)

for comm in communities:
    pattern = gpt.extract_pattern(comm)
    save_pattern(pattern)
```

### Option C: ìµœê³  í’ˆì§ˆ (4ì‹œê°„)
```python
# SBERT + HDBSCAN + GPT ê²€ì¦
model = SentenceTransformer('ko-sroberta')
embeddings = model.encode(beliefs)

reducer = umap.UMAP(n_components=10)
reduced = reducer.fit_transform(embeddings)

clusterer = hdbscan.HDBSCAN(min_cluster_size=15)
clusters = clusterer.fit_predict(reduced)

for cluster_id in unique(clusters):
    members = beliefs[clusters == cluster_id]
    pattern = gpt4.analyze_cluster(members)
    save_pattern(cluster_id, pattern)
```

---

## ğŸ”¥ ë‚´ ì¶”ì²œ: "3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ"

```python
# Stage 1: í‚¤ì›Œë“œë¡œ ëŒ€ë¶„ë¥˜ (ë¹ ë¦„)
broad_frames = keyword_classify(beliefs)  # 5-10ê°œ í”„ë ˆì„

# Stage 2: ê° í”„ë ˆì„ ë‚´ì—ì„œ Embedding í´ëŸ¬ìŠ¤í„°ë§ (ì •í™•)
for frame in broad_frames:
    embeddings = embed(frame.beliefs)
    sub_clusters = cluster(embeddings, threshold=0.75)

    # Stage 3: GPTë¡œ ìµœì¢… íŒ¨í„´ ì¶”ì¶œ (í•´ì„ ê°€ëŠ¥)
    for cluster in sub_clusters:
        pattern = gpt.extract_common_pattern(cluster)
        save_pattern(frame.name, pattern.name, pattern.description)
```

**ì™œ?**
1. **í‚¤ì›Œë“œ ë¶„ë¥˜**: ê³„ì‚° ë¹„ìš© 0, ëª…í™•í•œ ëŒ€ë¶„ë¥˜
2. **Embedding**: ì˜ë¯¸ ìœ ì‚¬ë„ë¡œ ì •ë°€ ë¶„ë¥˜
3. **GPT**: ì¸ê°„ì´ ì´í•´ ê°€ëŠ¥í•œ íŒ¨í„´ ì„œìˆ 

**ì˜ˆìƒ ê²°ê³¼:**
- 5-10ê°œ ëŒ€í”„ë ˆì„ (ë¯¼ì£¼ë‹¹ ë¹„íŒ, ê³„ì—„ ì •ë‹¹í™”, ì¢ŒíŒŒ ë¹„ë‚œ ë“±)
- ê° í”„ë ˆì„ë‹¹ 3-5ê°œ ì„¸ë¶€ íŒ¨í„´
- ì´ 15-50ê°œ ì˜ë¯¸ìˆëŠ” íŒ¨í„´

ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í• ê¹Œìš”?
