#!/usr/bin/env python3
"""í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¶„ì„ ë° ìµœì  ë°©ì•ˆ ì œì‹œ"""
import os
import numpy as np
from supabase import create_client
from dotenv import load_dotenv
from collections import Counter

load_dotenv()

supabase = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_SERVICE_KEY')
)

print("=" * 80)
print("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¶„ì„")
print("=" * 80)

# 1. ì „ì²´ ë…¼ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
logics = supabase.table('logic_repository')\
    .select('id, core_argument, context_issue, keywords, vector_embedding, created_at')\
    .not_.is_('vector_embedding', 'null')\
    .order('created_at', desc=False)\
    .limit(100)\
    .execute()

print(f"\nğŸ“ ë¶„ì„ ëŒ€ìƒ: {len(logics.data)}ê°œ ë…¼ë¦¬\n")

# 2. Context_issue ë¶„í¬ í™•ì¸
context_issues = [l.get('context_issue') for l in logics.data if l.get('context_issue')]
context_counter = Counter(context_issues)

print("ğŸ·ï¸  Context_issue ë¶„í¬ (ìƒìœ„ 10ê°œ):")
for issue, count in context_counter.most_common(10):
    print(f"   {count:2d}ê°œ: {issue[:60]}")

print(f"\n   ì´ {len(context_counter)}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ context_issue")
print(f"   í‰ê· : {len(context_issues)/len(context_counter):.1f}ê°œ/ì´ìŠˆ")

# 3. í‚¤ì›Œë“œ ì¤‘ë³µë„ í™•ì¸
all_keywords = []
for l in logics.data:
    if l.get('keywords'):
        all_keywords.extend(l['keywords'])

keyword_counter = Counter(all_keywords)
print(f"\nğŸ”‘ í‚¤ì›Œë“œ ì¤‘ë³µë„ (ìƒìœ„ 15ê°œ):")
for kw, count in keyword_counter.most_common(15):
    print(f"   {count:2d}íšŒ: {kw}")

# 4. ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„ (ìƒ˜í”Œ)
print(f"\nğŸ§® ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„ (ìƒ˜í”Œ 20ê°œ):")
print("   ê°™ì€ í‚¤ì›Œë“œë¥¼ ê³µìœ í•˜ëŠ” ë…¼ë¦¬ ìŒì˜ ìœ ì‚¬ë„:\n")

similarities = []
for i in range(min(20, len(logics.data))):
    for j in range(i+1, min(i+10, len(logics.data))):
        l1 = logics.data[i]
        l2 = logics.data[j]

        # ê³µí†µ í‚¤ì›Œë“œê°€ ìˆëŠ”ê°€?
        kw1 = set(l1.get('keywords', []))
        kw2 = set(l2.get('keywords', []))
        common_kw = kw1 & kw2

        if not common_kw:
            continue

        # ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
        try:
            v1 = np.array(l1['vector_embedding'], dtype=float)
            v2 = np.array(l2['vector_embedding'], dtype=float)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            similarities.append(similarity)

            if len(similarities) <= 10:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"   ìœ ì‚¬ë„ {similarity:.3f} | ê³µí†µí‚¤ì›Œë“œ: {list(common_kw)[:2]}")
                print(f"      A: {l1['core_argument'][:50]}...")
                print(f"      B: {l2['core_argument'][:50]}...")
                print()
        except Exception as e:
            continue

if similarities:
    print(f"   ğŸ“ˆ í†µê³„:")
    print(f"      í‰ê·  ìœ ì‚¬ë„: {np.mean(similarities):.3f}")
    print(f"      ì¤‘ì•™ê°’: {np.median(similarities):.3f}")
    print(f"      ìµœì†Œê°’: {np.min(similarities):.3f}")
    print(f"      ìµœëŒ€ê°’: {np.max(similarities):.3f}")
    print(f"      í‘œì¤€í¸ì°¨: {np.std(similarities):.3f}")

# 5. ìµœì  ì„ê³„ê°’ ì œì•ˆ
print("\n" + "=" * 80)
print("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ë° ì œì•ˆ")
print("=" * 80)

if similarities:
    avg_sim = np.mean(similarities)
    std_sim = np.std(similarities)

    print(f"\n1ï¸âƒ£  ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜:")
    print(f"   í˜„ì¬ ì„ê³„ê°’: 0.75 (ë„ˆë¬´ ë†’ìŒ)")
    print(f"   ì‹¤ì œ í‰ê· : {avg_sim:.3f}")
    print(f"   ì œì•ˆ ì„ê³„ê°’: {avg_sim - 0.5*std_sim:.3f} ~ {avg_sim:.3f}")

print(f"\n2ï¸âƒ£  Context_issue ê¸°ë°˜:")
print(f"   ì´ {len(context_counter)}ê°œ ì„œë¡œ ë‹¤ë¥¸ ì´ìŠˆ")
print(f"   ë¬¸ì œ: ë„ˆë¬´ ì„¸ë¶„í™”ë˜ì–´ ìˆìŒ")
print(f"   ì œì•ˆ: GPT í”„ë¡¬í”„íŠ¸ ìˆ˜ì • â†’ ì§§ì€ í‚¤ì›Œë“œ")

print(f"\n3ï¸âƒ£  í‚¤ì›Œë“œ ê¸°ë°˜:")
print(f"   ìƒìœ„ í‚¤ì›Œë“œ ì¤‘ë³µë„ê°€ ë†’ìŒ")
print(f"   ì œì•ˆ: ê³µí†µ í‚¤ì›Œë“œ 2ê°œ ì´ìƒ â†’ ê°™ì€ í´ëŸ¬ìŠ¤í„°")

print("\n" + "=" * 80)
print("ğŸ¯ ìµœì  í´ëŸ¬ìŠ¤í„°ë§ ì „ëµ ì œì•ˆ")
print("=" * 80)

print("""
ì „ëµ A: í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ (ê¶Œì¥)
  1. ê³µí†µ í‚¤ì›Œë“œ 2ê°œ ì´ìƒ â†’ ì¦‰ì‹œ ê°™ì€ í´ëŸ¬ìŠ¤í„°
  2. ë²¡í„° ìœ ì‚¬ë„ > 0.5 â†’ ê°™ì€ í´ëŸ¬ìŠ¤í„°
  3. Context_issue ì¼ì¹˜ â†’ ê°™ì€ í´ëŸ¬ìŠ¤í„°
  â†’ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ë¬¶ìŒ

ì „ëµ B: ì ì§„ì  ì„±ì¥ + ì£¼ê¸°ì  ì¬ì¡°ì •
  1. ìƒˆ ë…¼ë¦¬ ì¶”ê°€ ì‹œ ëŠìŠ¨í•œ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ (ìœ ì‚¬ë„ 0.4)
  2. ë§¤ì¼ ë°¤ ì „ì²´ ì¬í´ëŸ¬ìŠ¤í„°ë§ (K-means ë“±)
  3. í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ë²¡í„° ì¬ê³„ì‚°

ì „ëµ C: 2ë‹¨ê³„ í´ëŸ¬ìŠ¤í„°ë§
  1. í‚¤ì›Œë“œ ê¸°ë°˜ 1ì°¨ ê·¸ë£¹í™” (ë¹ ë¦„)
  2. ê° ê·¸ë£¹ ë‚´ì—ì„œ ë²¡í„° ê¸°ë°˜ ì„¸ë¶„í™” (ì •í™•í•¨)
""")