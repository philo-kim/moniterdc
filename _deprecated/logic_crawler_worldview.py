#!/usr/bin/env python3
"""
DC Gallery Logic Crawler - Worldview Analysis Version
ì„¸ê³„ê´€ ë¶„ì„: ì£¼ì²´ ì¤‘ì‹¬ ì ‘ê·¼
"""

import os
import asyncio
import json
from datetime import datetime, timezone
from dotenv import load_dotenv
from supabase import create_client
from openai import AsyncOpenAI
import aiohttp
from bs4 import BeautifulSoup
from typing import Dict, List, Optional
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class WorldviewCrawler:
    """ì„¸ê³„ê´€ ë¶„ì„ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_KEY')
        )
        self.openai_client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    async def analyze_worldview_with_gpt(self, title: str, content: str) -> Optional[Dict]:
        """
        ì„¸ê³„ê´€ ë¶„ì„: ì£¼ì²´, ì†ì„±, ê´€ê³„ ì¶”ì¶œ
        """
        if not content or len(content.strip()) < 20:
            return None

        try:
            response = await self.openai_client.chat.completions.create(
                model=os.getenv('GPT_ANALYSIS_MODEL', 'gpt-5-mini'),
                messages=[
                    {
                        'role': 'system',
                        'content': '''ë‹¹ì‹ ì€ í•œêµ­ ì •ì¹˜ ì„¸ê³„ê´€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ëª©ì **: DCê°¤ëŸ¬ë¦¬ ê¸€ì´ êµ¬ì„±í•˜ëŠ” **ì„¸ê³„ê´€**ì„ ë¶„ì„í•˜ì„¸ìš”.

ì„¸ê³„ê´€ì€ ë‹¤ìŒìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:
1. **ì£¼ì²´ë“¤** (actors): ëˆ„êµ¬ì— ëŒ€í•œ ì´ì•¼ê¸°ì¸ê°€?
2. **ì†ì„±/í‰ê°€** (attributes): ê·¸ë“¤ì„ ì–´ë–»ê²Œ ê·œì •í•˜ëŠ”ê°€?
3. **ê´€ê³„** (relations): ì£¼ì²´ë“¤ ê°„ì˜ ì—°ê²°ì€?

ì˜ˆì‹œ:
- ê¸€: "ì¤‘êµ­ì¸ ë¬´ë¹„ìë¡œ ì¡°ì„ ì¡± ë¬´ë”ê¸° ì…êµ­, ë¯¼ì£¼ë‹¹ì´ í—ˆìš©"
  â†’ ì£¼ì²´1: ë¯¼ì£¼ë‹¹ (í‰ê°€: ì¹œì¤‘ì •ì±… ì¶”ì§„)
  â†’ ì£¼ì²´2: ì¤‘êµ­/ì¡°ì„ ì¡± (í‰ê°€: ì•ˆë³´ìœ„í˜‘)
  â†’ ê´€ê³„: ë¯¼ì£¼ë‹¹ì´ ì¤‘êµ­ì¸ ìœ ì…ì„ í—ˆìš©í•¨

- ê¸€: "ìœ¤ì„ì—´ ëŒ€í†µë ¹ í•œë¯¸ì •ìƒíšŒë‹´, ë™ë§¹ ê°•í™”"
  â†’ ì£¼ì²´1: ìœ¤ì„ì—´ (í‰ê°€: ì™¸êµì„±ê³¼, ë™ë§¹ê°•í™”)
  â†’ ì£¼ì²´2: ë¯¸êµ­ (í‰ê°€: ìš°ë°©)
  â†’ ê´€ê³„: ìœ¤ì„ì—´ì´ ë¯¸êµ­ê³¼ ë™ë§¹ì„ ê°•í™”í•¨

ë‹¤ìŒ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "core_argument": "í•µì‹¬ ì£¼ì¥ í•œ ë¬¸ì¥",
  "subjects": [
    {
      "name": "ì£¼ì²´ ì´ë¦„ (ì˜ˆ: ë¯¼ì£¼ë‹¹, ì´ì¬ëª…, ìœ¤ì„ì—´, ì¤‘êµ­, ë¯¸êµ­)",
      "attributes": ["ì†ì„±1", "ì†ì„±2"],
      "sentiment": "positive/negative/neutral"
    }
  ],
  "relations": [
    {
      "subject1": "ì£¼ì²´1",
      "relation_type": "ê´€ê³„ ìœ í˜• (ì˜ˆ: ê²°íƒ, ê³µê²©, ì˜¹í˜¸, í—ˆìš©)",
      "subject2": "ì£¼ì²´2"
    }
  ],
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "context_issue": "êµ¬ì²´ì  ì‚¬ê±´/ì´ìŠˆ",
  "threat_level": 5,
  "evidence_quality": 5
}

**ì¤‘ìš”**:
- subjects: ìµœëŒ€ 3ê°œê¹Œì§€, í•µì‹¬ ì£¼ì²´ë§Œ
- attributes: ê° ì£¼ì²´ë‹¹ 1-3ê°œ, ê°„ê²°í•˜ê²Œ
- sentiment: ì´ ê¸€ì—ì„œ í•´ë‹¹ ì£¼ì²´ë¥¼ ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì ìœ¼ë¡œ ë¬˜ì‚¬í•˜ëŠ”ì§€
- relations: ì£¼ì²´ë“¤ ê°„ì˜ ì—°ê²°ì„ ëª…ì‹œ

ë‹¨ìˆœ ìš•ì„¤/ì¡°ë¡±ë§Œ ìˆìœ¼ë©´ null ë°˜í™˜.'''
                    },
                    {
                        'role': 'user',
                        'content': f'ì œëª©: {title}\n\në³¸ë¬¸: {content[:1000]}'
                    }
                ]
            )

            analysis_text = response.choices[0].message.content.strip()

            if "```json" in analysis_text:
                analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
            elif analysis_text.lower() == 'null':
                return None

            return json.loads(analysis_text)

        except Exception as e:
            logger.error(f"ì„¸ê³„ê´€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return None

    async def fetch_post_content(self, post_url: str) -> Optional[str]:
        """ê²Œì‹œê¸€ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {'User-Agent': 'Mozilla/5.0'}
                async with session.get(post_url, headers=headers, timeout=10) as response:
                    if response.status != 200:
                        return None

                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    content_div = soup.select_one('.write_div')
                    if not content_div:
                        return None

                    content = content_div.get_text(strip=True, separator='\n')
                    return content

        except Exception as e:
            logger.error(f"Error fetching {post_url}: {str(e)}")
            return None

    async def save_to_repository(self, post: Dict, content: str, analysis: Dict):
        """ì €ì¥"""
        try:
            logic_data = {
                'logic_type': post['logic_type'],
                'source_gallery': post['gallery_id'],
                'ai_classification': 'ê³µê²©ì ' if analysis.get('threat_level', 0) > 5 else 'ì¤‘ë¦½ì ',
                'core_argument': analysis.get('core_argument', ''),
                'keywords': analysis.get('keywords', []),
                'evidence_quality': analysis.get('evidence_quality', 5),
                'threat_level': analysis.get('threat_level', 3),
                'original_title': post['title'],
                'original_content': content,
                'original_url': post['post_url'],
                'original_post_num': post['post_num'],
                'effectiveness_score': analysis.get('evidence_quality', 5),
                'context_issue': analysis.get('context_issue'),
                'worldview_data': {  # ìƒˆë¡œìš´ í•„ë“œ
                    'subjects': analysis.get('subjects', []),
                    'relations': analysis.get('relations', [])
                },
                'usage_count': 0,
                'success_count': 0,
                'is_active': True,
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }

            result = self.supabase.table('logic_repository').insert(logic_data).execute()

            if result.data:
                logger.info(f"âœ… ë…¼ë¦¬ ì €ì¥: {post['title'][:30]}...")

                # ì£¼ì²´ë³„ í´ëŸ¬ìŠ¤í„° ìƒì„±/ì—…ë°ì´íŠ¸
                await self.update_subject_clusters(result.data[0]['id'], analysis)

                return result.data[0]['id']

        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return None

    async def update_subject_clusters(self, logic_id: str, analysis: Dict):
        """ì£¼ì²´ë³„ í´ëŸ¬ìŠ¤í„° ì—…ë°ì´íŠ¸"""
        subjects = analysis.get('subjects', [])

        for subject in subjects:
            subject_name = subject.get('name')
            if not subject_name:
                continue

            # ì£¼ì²´ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° ë˜ëŠ” ìƒì„±
            cluster_result = self.supabase.table('subject_clusters').select('*').eq('subject_name', subject_name).execute()

            if cluster_result.data:
                cluster_id = cluster_result.data[0]['id']
            else:
                # ìƒˆ ì£¼ì²´ í´ëŸ¬ìŠ¤í„° ìƒì„±
                new_cluster = self.supabase.table('subject_clusters').insert({
                    'subject_name': subject_name,
                    'total_mentions': 0,
                    'positive_count': 0,
                    'negative_count': 0,
                    'neutral_count': 0,
                    'common_attributes': [],
                    'first_seen': datetime.now(timezone.utc).isoformat(),
                    'last_seen': datetime.now(timezone.utc).isoformat()
                }).execute()

                cluster_id = new_cluster.data[0]['id']

            # ì£¼ì²´-ë…¼ë¦¬ ì—°ê²°
            self.supabase.table('subject_logic_mapping').insert({
                'subject_cluster_id': cluster_id,
                'logic_id': logic_id,
                'sentiment': subject.get('sentiment', 'neutral'),
                'attributes': subject.get('attributes', [])
            }).execute()

            # í´ëŸ¬ìŠ¤í„° í†µê³„ ì—…ë°ì´íŠ¸
            sentiment = subject.get('sentiment', 'neutral')
            self.supabase.rpc('increment_subject_cluster_stats', {
                'cluster_id': cluster_id,
                'sentiment_type': sentiment
            }).execute()

    async def crawl_gallery(self, gallery_id: str, logic_type: str, limit: int = 10):
        """ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§"""
        logger.info(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {gallery_id} ({logic_type})")

        base_url = f'https://gall.dcinside.com/board/lists/?id={gallery_id}'

        try:
            async with aiohttp.ClientSession() as session:
                headers = {'User-Agent': 'Mozilla/5.0'}
                async with session.get(base_url, headers=headers) as response:
                    if response.status != 200:
                        logger.error(f"Failed to fetch {base_url}")
                        return

                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    posts = soup.select('.gall_list tbody tr.ub-content')[:limit]

                    for post_elem in posts:
                        try:
                            title_elem = post_elem.select_one('.gall_tit a')
                            if not title_elem:
                                continue

                            title = title_elem.get_text(strip=True)
                            post_num = title_elem['href'].split('no=')[1].split('&')[0]
                            post_url = f'https://gall.dcinside.com/board/view/?id={gallery_id}&no={post_num}'

                            # ì¤‘ë³µ ì²´í¬
                            existing = self.supabase.table('logic_repository').select('id').eq('original_post_num', post_num).execute()
                            if existing.data:
                                continue

                            # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                            content = await self.fetch_post_content(post_url)
                            if not content:
                                continue

                            # ì„¸ê³„ê´€ ë¶„ì„
                            analysis = await self.analyze_worldview_with_gpt(title, content)
                            if not analysis:
                                continue

                            # ì €ì¥
                            post_data = {
                                'gallery_id': gallery_id,
                                'logic_type': logic_type,
                                'title': title,
                                'post_num': post_num,
                                'post_url': post_url
                            }

                            await self.save_to_repository(post_data, content, analysis)
                            await asyncio.sleep(1)

                        except Exception as e:
                            logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                            continue

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")


async def main():
    crawler = WorldviewCrawler()

    # ê³µê²© ë…¼ë¦¬ ìˆ˜ì§‘ (ë¯¸ì •ê°¤)
    await crawler.crawl_gallery('uspolitics', 'attack', limit=5)

    logger.info("âœ… í¬ë¡¤ë§ ì™„ë£Œ")


if __name__ == '__main__':
    asyncio.run(main())