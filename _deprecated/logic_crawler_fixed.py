#!/usr/bin/env python3
"""
ì™„ì „íˆ ìƒˆë¡œìš´ Logic Defense í¬ë¡¤ëŸ¬ - ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶¤
"""

import os
import sys
import asyncio
import json
import random
from datetime import datetime, timezone
from typing import List, Dict, Optional
from dotenv import load_dotenv

import aiohttp
from bs4 import BeautifulSoup
from supabase import create_client, Client
from openai import AsyncOpenAI
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# DCê°¤ëŸ¬ë¦¬ ì„¤ì • (ê³µê²© ë…¼ë¦¬ë§Œ ìˆ˜ì§‘)
GALLERIES = {
    'uspolitics': {
        'id': 'uspolitics',
        'name': 'ë¯¸êµ­ì •ì¹˜',
        'logic_type': 'attack',
        'url': 'https://gall.dcinside.com/mgallery/board/lists',
        'is_mgallery': True
    }
    # ë°©ì–´ ë…¼ë¦¬ëŠ” ì´ì œ ì‚¬ìš©ìê°€ ëŒ€ì‹œë³´ë“œì—ì„œ ì§ì ‘ ì‘ì„±
}

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

class FixedLogicCrawler:
    def __init__(self):
        self.supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        self.session = aiohttp.ClientSession(headers=headers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            await asyncio.sleep(random.uniform(0.5, 1.0))
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status} for {url}")
        except Exception as e:
            logger.error(f"Error fetching {url}: {str(e)}")
        return None

    def parse_concept_post(self, post_elem, gallery: Dict) -> Optional[Dict]:
        """ê°œë…ê¸€ ë°ì´í„° íŒŒì‹±"""
        try:
            # ê²Œì‹œê¸€ ë²ˆí˜¸
            num_elem = post_elem.select_one('.gall_num')
            if not num_elem or num_elem.text.strip() in ['ê³µì§€', 'AD']:
                return None

            try:
                post_num = int(num_elem.text.strip())
            except ValueError:
                return None

            # ì œëª©ê³¼ ë§í¬
            title_elem = post_elem.select_one('.gall_tit a')
            if not title_elem:
                return None

            title = title_elem.text.strip()
            href = title_elem.get('href', '')

            # URL ìƒì„±
            if gallery.get('is_mgallery', False):
                if href.startswith('/'):
                    post_url = f"https://gall.dcinside.com{href}"
                else:
                    post_url = href
            else:
                post_url = f"https://gall.dcinside.com{href}"

            # ì‘ì„±ì
            writer_elem = post_elem.select_one('.gall_writer')
            author = writer_elem.get('data-nick', 'Unknown') if writer_elem else 'Unknown'

            return {
                'post_num': post_num,
                'title': title,
                'author': author,
                'post_url': post_url,
                'gallery_id': gallery['id'],
                'logic_type': gallery['logic_type']
            }

        except Exception as e:
            logger.error(f"Error parsing post: {str(e)}")
            return None

    async def fetch_concept_posts(self, gallery_key: str, max_pages: int = 15) -> List[Dict]:
        """ê°œë…ê¸€ ìˆ˜ì§‘"""
        gallery = GALLERIES.get(gallery_key)
        if not gallery:
            return []

        posts = []
        logger.info(f"ğŸ¯ {gallery['name']} ê°œë…ê¸€ ìˆ˜ì§‘ ì‹œì‘...")

        for page in range(1, max_pages + 1):
            try:
                # ê°œë…ê¸€ í˜ì´ì§€ URL
                url = f"{gallery['url']}?id={gallery['id']}&page={page}&exception_mode=recommend"

                html = await self.fetch_page(url)
                if not html:
                    continue

                soup = BeautifulSoup(html, 'html.parser')
                post_elements = soup.select('tr.us-post')  # ê°œë…ê¸€

                page_posts = []
                for post_elem in post_elements:
                    post_data = self.parse_concept_post(post_elem, gallery)
                    if post_data:
                        page_posts.append(post_data)

                posts.extend(page_posts)
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page}: {len(page_posts)}ê°œ")

                if not page_posts:  # ë” ì´ìƒ ê°œë…ê¸€ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                    break

            except Exception as e:
                logger.error(f"Error crawling page {page}: {str(e)}")
                continue

        logger.info(f"âœ… {gallery['name']}: ì´ {len(posts)}ê°œ ê°œë…ê¸€ ìˆ˜ì§‘")
        return posts

    async def fetch_post_content(self, post_url: str) -> Optional[str]:
        """ê²Œì‹œê¸€ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            html = await self.fetch_page(post_url)
            if not html:
                return None

            soup = BeautifulSoup(html, 'html.parser')

            # ë³¸ë¬¸ ì°¾ê¸°
            content_elem = soup.select_one('.write_div') or soup.select_one('.writing_view_box')
            if content_elem:
                content = content_elem.get_text(strip=True, separator='\n')
                return content  # ì „ì²´ ë³¸ë¬¸ ì €ì¥ (ë§¥ë½ íŒŒì•… ìœ„í•´)

        except Exception as e:
            logger.error(f"Error fetching content from {post_url}: {str(e)}")

        return None

    async def analyze_logic_with_gpt5(self, title: str, content: str) -> Optional[Dict]:
        """GPT-5ë¡œ ë…¼ë¦¬ ë¶„ì„"""
        if not content or len(content.strip()) < 20:
            return None

        try:
            # GPT-5 temperature ê¸°ë³¸ê°’(1) ì‚¬ìš©
            response = await self.openai_client.chat.completions.create(
                model=os.getenv('GPT_ANALYSIS_MODEL', 'gpt-5-mini'),
                messages=[
                    {
                        'role': 'system',
                        'content': '''ë‹¹ì‹ ì€ í•œêµ­ ì •ì¹˜ ë…¼ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. DCê°¤ëŸ¬ë¦¬ ê°œë…ê¸€ì„ ë¶„ì„í•˜ì—¬ **ì´ ê¸€ì´ ì–´ë–¤ ì •ì¹˜ì  í”„ë ˆì„/ë‚´ëŸ¬í‹°ë¸Œë¥¼ êµ¬ì„±í•˜ëŠ”ì§€** íŒŒì•…í•˜ì„¸ìš”.

**í•µì‹¬ ëª©ì **: ì´ ê¸€ì´ ì–´ë–¤ **ì™œê³¡ëœ ì„¸ê³„ê´€**ì„ ë§Œë“¤ì–´ë‚´ëŠ” ë° ê¸°ì—¬í•˜ëŠ”ê°€?

ì •ì¹˜ì  í”„ë ˆì„/ë‚´ëŸ¬í‹°ë¸Œ ì˜ˆì‹œ:
- "ë¯¼ì£¼ë‹¹=ì¹œì¤‘=êµ­ê°€ì•ˆë³´ìœ„í˜‘" â†’ ì¤‘êµ­ì¸ë¬´ë¹„ì, ì¤‘êµ­ì¸ë„ë§, ê°„ì²©ì˜í˜¹, ì¹œì¤‘ì™¸êµ ë“±ì˜ ê¸€ë“¤ì´ ì´ í”„ë ˆì„ êµ¬ì„±
- "ì´ì¬ëª…=ë²”ì£„ì=ë¯¼ì£¼ë‹¹ë¶•ê´´" â†’ ê¹€í˜œê²½ì‡¼í•‘, ëŒ€ì¥ë™ì˜í˜¹, ìœ„ì¦êµì‚¬ ë“±ì˜ ê¸€ë“¤ì´ ì´ í”„ë ˆì„ êµ¬ì„±
- "ìœ¤ì„ì—´=êµ­ê°€ìˆ˜í˜¸ì" â†’ ì•ˆë³´ê°•í™”, ë¶í•œëŒ€ì‘, ë™ë§¹ê°•í™” ë“±ì˜ ê¸€ë“¤ì´ ì´ í”„ë ˆì„ êµ¬ì„±
- "í•œêµ­ì–¸ë¡ =í¸íŒŒ=ì¡°ì‘" â†’ KBSí¸íŒŒ, MBCì™œê³¡, ì–¸ë¡ ê°œí˜í•„ìš” ë“±ì˜ ê¸€ë“¤ì´ ì´ í”„ë ˆì„ êµ¬ì„±

ë‹¤ìŒ JSON êµ¬ì¡°ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
  "core_argument": "í•µì‹¬ ì£¼ì¥ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "ai_classification": "ê³µê²©ì /ë°©ì–´ì /ì¤‘ë¦½ì ",
  "evidence_quality": 5,
  "threat_level": 3,
  "effectiveness_score": 7,
  "political_frame": "ì´ ê¸€ì´ êµ¬ì„±í•˜ëŠ” ì •ì¹˜ì  í”„ë ˆì„/ë‚´ëŸ¬í‹°ë¸Œ (ì˜ˆ: ë¯¼ì£¼ë‹¹=ì¹œì¤‘=ì•ˆë³´ìœ„í˜‘, ì´ì¬ëª…=ë²”ì£„ì, ìœ¤ì„ì—´=êµ­ê°€ìˆ˜í˜¸ì, í•œêµ­ì–¸ë¡ =í¸íŒŒ)",
  "context_issue": "êµ¬ì²´ì  ì´ìŠˆ/ì‚¬ê±´ (ì˜ˆ: ì¤‘êµ­ì¸ë¬´ë¹„ì, ê¹€í˜œê²½ì‡¼í•‘, KBSí¸íŒŒë³´ë„)",
  "distortion_pattern": "ì‚¬ìš©ëœ ì™œê³¡ ê¸°ë²• (ì˜ˆ: ë§¥ë½ì œê±°, ê³¼ì¥, ì¸ì‹ ê³µê²©, í—ˆìœ„ì—°ê´€)"
}

**ì¤‘ìš”**:
- political_frame: ì—¬ëŸ¬ ê¸€ë“¤ì´ ê³µìœ í•  ìˆ˜ ìˆëŠ” **í° ë‚´ëŸ¬í‹°ë¸Œ/ì„¸ê³„ê´€**
- context_issue: ì´ ê¸€ì´ ë‹¤ë£¨ëŠ” **êµ¬ì²´ì  ì‚¬ê±´/ì´ìŠˆ**
- ê°™ì€ political_frameì„ ê°€ì§„ ê¸€ë“¤ì€ ë‹¤ë¥¸ context_issueë¥¼ ë‹¤ë£¨ë”ë¼ë„ ê°™ì€ ì™œê³¡ëœ ì„¸ê³„ê´€ì„ êµ¬ì„±í•¨

ë‹¨ìˆœ ìš•ì„¤/ì¡°ë¡±ë§Œ ìˆê³  êµ¬ì²´ì  ì£¼ì¥ì´ ì—†ìœ¼ë©´ nullì„ ë°˜í™˜í•˜ì„¸ìš”.'''
                    },
                    {
                        'role': 'user',
                        'content': f'ì œëª©: {title}\n\në³¸ë¬¸: {content}'
                    }
                ]
                # temperature íŒŒë¼ë¯¸í„° ì œê±° (GPT-5ëŠ” ê¸°ë³¸ê°’ë§Œ ì§€ì›)
            )

            analysis_text = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            if "```json" in analysis_text:
                analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
            elif analysis_text.lower() == 'null':
                return None

            try:
                return json.loads(analysis_text)
            except json.JSONDecodeError:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {analysis_text[:100]}...")
                return None

        except Exception as e:
            logger.error(f"ë…¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨ ({title[:30]}): {str(e)}")
            return None

    async def save_to_logic_repository(self, post: Dict, content: str, analysis: Dict):
        """ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì €ì¥"""
        try:
            # ì‹¤ì œ logic_repository í…Œì´ë¸” ì»¬ëŸ¼ì— ë§ì¶° ë°ì´í„° êµ¬ì„±
            logic_data = {
                'logic_type': post['logic_type'],  # attack/defense
                'source_gallery': post['gallery_id'],  # uspolitics/minjudang
                'ai_classification': analysis.get('ai_classification', 'ë¶„ì„í•„ìš”'),
                'core_argument': analysis.get('core_argument', ''),
                'keywords': analysis.get('keywords', []),
                'evidence_quality': analysis.get('evidence_quality', 5),
                'threat_level': analysis.get('threat_level', 3),
                'original_title': post['title'],
                'original_content': content,  # ì „ì²´ ë³¸ë¬¸ ì €ì¥ (ë§¥ë½ íŒŒì•… ìœ„í•´)
                'original_url': post['post_url'],
                'original_post_num': post['post_num'],
                'effectiveness_score': analysis.get('effectiveness_score', 5),
                'political_frame': analysis.get('political_frame'),  # ì •ì¹˜ì  í”„ë ˆì„/ë‚´ëŸ¬í‹°ë¸Œ
                'context_issue': analysis.get('context_issue'),  # ê´€ë ¨ ì‚¬ê±´/ì´ìŠˆ
                'distortion_pattern': analysis.get('distortion_pattern'),  # ì™œê³¡ íŒ¨í„´
                'usage_count': 0,
                'success_count': 0,
                'is_active': True,
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }

            # Supabaseì— ì €ì¥
            result = self.supabase.table('logic_repository').insert(logic_data).execute()

            if result.data:
                logger.info(f"âœ… ë…¼ë¦¬ ì €ì¥ ì„±ê³µ: {post['title'][:30]}...")
                return result.data[0]['id']
            else:
                logger.error(f"âŒ ë…¼ë¦¬ ì €ì¥ ì‹¤íŒ¨: {post['title'][:30]}...")
                return None

        except Exception as e:
            logger.error(f"ì €ì¥ ì˜¤ë¥˜ ({post['title'][:30]}): {str(e)}")
            return None

    async def create_embedding(self, title: str, content: str, core_argument: str) -> Optional[List[float]]:
        """ì„ë² ë”© ìƒì„±"""
        try:
            # ì œëª© + ë³¸ë¬¸ + í•µì‹¬ë…¼ë¦¬ë¥¼ ê²°í•©í•´ì„œ ì„ë² ë”©
            combined_text = f"{title}\n\n{content}\n\ní•µì‹¬ë…¼ë¦¬: {core_argument}"

            response = await self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=combined_text[:8000]  # í† í° ì œí•œ
            )

            return response.data[0].embedding

        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None

    async def save_embedding(self, logic_id: str, embedding: List[float]):
        """ì„ë² ë”©ì„ logic_repository.vector_embedding ì»¬ëŸ¼ì— ì €ì¥"""
        try:
            # logic_repository í…Œì´ë¸”ì˜ vector_embedding ì»¬ëŸ¼ì— ì§ì ‘ ì—…ë°ì´íŠ¸
            result = self.supabase.table('logic_repository').update({
                'vector_embedding': embedding
            }).eq('id', logic_id).execute()

            if result.data:
                logger.info(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {logic_id}")
            else:
                logger.error(f"âŒ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {logic_id}")

        except Exception as e:
            logger.error(f"ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨ ({logic_id}): {str(e)}")

    async def process_posts(self, posts: List[Dict], batch_size: int = 10):
        """ê²Œì‹œê¸€ë“¤ ì²˜ë¦¬"""
        logger.info(f"ğŸ“– {len(posts)}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹œì‘...")

        processed = 0
        for i in range(0, len(posts), batch_size):
            batch = posts[i:i+batch_size]

            for post in batch:
                try:
                    # 1. ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    content = await self.fetch_post_content(post['post_url'])
                    if not content:
                        continue

                    # 2. ë…¼ë¦¬ ë¶„ì„
                    analysis = await self.analyze_logic_with_gpt5(post['title'], content)
                    if not analysis:
                        continue

                    # 3. DB ì €ì¥
                    logic_id = await self.save_to_logic_repository(post, content, analysis)
                    if not logic_id:
                        continue

                    # 4. ì„ë² ë”© ìƒì„± ë° ì €ì¥
                    embedding = await self.create_embedding(
                        post['title'], content, analysis.get('core_argument', '')
                    )
                    if embedding:
                        await self.save_embedding(logic_id, embedding)

                    processed += 1
                    logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ {processed}: {post['title'][:30]}...")

                except Exception as e:
                    logger.error(f"ê²Œì‹œê¸€ ì²˜ë¦¬ ì‹¤íŒ¨ ({post['title'][:30]}): {str(e)}")

            logger.info(f"ğŸ“¦ ë°°ì¹˜ {(i//batch_size)+1} ì™„ë£Œ")

        return processed

    async def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Fixed Logic Defense í¬ë¡¤ëŸ¬ ì‹œì‘!")

        total_processed = 0

        # ë¯¸êµ­ì •ì¹˜ ê°¤ëŸ¬ë¦¬ ì²˜ë¦¬ (ê³µê²© ë…¼ë¦¬ ìˆ˜ì§‘)
        gallery_key = 'uspolitics'
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ {GALLERIES[gallery_key]['name']} ê°¤ëŸ¬ë¦¬")
        logger.info(f"{'='*60}")

        try:
            # ê°œë…ê¸€ ìˆ˜ì§‘
            posts = await self.fetch_concept_posts(gallery_key, max_pages=5)

            if posts:
                # ì²˜ë¦¬ (20ê°œ)
                processed = await self.process_posts(posts[:20])
                total_processed += processed
                logger.info(f"ğŸ‰ {GALLERIES[gallery_key]['name']}: {processed}ê°œ ì™„ë£Œ")
            else:
                logger.warning(f"{GALLERIES[gallery_key]['name']}: ê°œë…ê¸€ ì—†ìŒ")

        except Exception as e:
            logger.error(f"{gallery_key} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

        logger.info(f"\nğŸ‰ ì´ {total_processed}ê°œ ë…¼ë¦¬ ë¶„ì„ ì™„ë£Œ!")

async def main():
    if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_API_KEY]):
        logger.error("âŒ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½")
        sys.exit(1)

    async with FixedLogicCrawler() as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())