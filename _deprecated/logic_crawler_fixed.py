#!/usr/bin/env python3
"""
ÏôÑÏ†ÑÌûà ÏÉàÎ°úÏö¥ Logic Defense ÌÅ¨Î°§Îü¨ - Ïã§Ï†ú DB Ïä§ÌÇ§ÎßàÏóê ÎßûÏ∂§
"""

import os
import sys
import asyncio
import json
import random
from datetime import datetime, timezone
from typing import List, Dict, Optional
from dotenv import load_dotenv

import aiohttp
from bs4 import BeautifulSoup
from supabase import create_client, Client
from openai import AsyncOpenAI
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ÌôòÍ≤ΩÎ≥ÄÏàò
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# DCÍ∞§Îü¨Î¶¨ ÏÑ§Ï†ï (Í≥µÍ≤© ÎÖºÎ¶¨Îßå ÏàòÏßë)
GALLERIES = {
    'uspolitics': {
        'id': 'uspolitics',
        'name': 'ÎØ∏Íµ≠Ï†ïÏπò',
        'logic_type': 'attack',
        'url': 'https://gall.dcinside.com/mgallery/board/lists',
        'is_mgallery': True
    }
    # Î∞©Ïñ¥ ÎÖºÎ¶¨Îäî Ïù¥Ï†ú ÏÇ¨Ïö©ÏûêÍ∞Ä ÎåÄÏãúÎ≥¥ÎìúÏóêÏÑú ÏßÅÏ†ë ÏûëÏÑ±
}

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

class FixedLogicCrawler:
    def __init__(self):
        self.supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        self.session = aiohttp.ClientSession(headers=headers)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_page(self, url: str) -> Optional[str]:
        """ÌéòÏù¥ÏßÄ HTML Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            await asyncio.sleep(random.uniform(0.5, 1.0))
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status} for {url}")
        except Exception as e:
            logger.error(f"Error fetching {url}: {str(e)}")
        return None

    def parse_concept_post(self, post_elem, gallery: Dict) -> Optional[Dict]:
        """Í∞úÎÖêÍ∏Ä Îç∞Ïù¥ÌÑ∞ ÌååÏã±"""
        try:
            # Í≤åÏãúÍ∏Ä Î≤àÌò∏
            num_elem = post_elem.select_one('.gall_num')
            if not num_elem or num_elem.text.strip() in ['Í≥µÏßÄ', 'AD']:
                return None

            try:
                post_num = int(num_elem.text.strip())
            except ValueError:
                return None

            # Ï†úÎ™©Í≥º ÎßÅÌÅ¨
            title_elem = post_elem.select_one('.gall_tit a')
            if not title_elem:
                return None

            title = title_elem.text.strip()
            href = title_elem.get('href', '')

            # URL ÏÉùÏÑ±
            if gallery.get('is_mgallery', False):
                if href.startswith('/'):
                    post_url = f"https://gall.dcinside.com{href}"
                else:
                    post_url = href
            else:
                post_url = f"https://gall.dcinside.com{href}"

            # ÏûëÏÑ±Ïûê
            writer_elem = post_elem.select_one('.gall_writer')
            author = writer_elem.get('data-nick', 'Unknown') if writer_elem else 'Unknown'

            return {
                'post_num': post_num,
                'title': title,
                'author': author,
                'post_url': post_url,
                'gallery_id': gallery['id'],
                'logic_type': gallery['logic_type']
            }

        except Exception as e:
            logger.error(f"Error parsing post: {str(e)}")
            return None

    async def fetch_concept_posts(self, gallery_key: str, max_pages: int = 15) -> List[Dict]:
        """Í∞úÎÖêÍ∏Ä ÏàòÏßë"""
        gallery = GALLERIES.get(gallery_key)
        if not gallery:
            return []

        posts = []
        logger.info(f"üéØ {gallery['name']} Í∞úÎÖêÍ∏Ä ÏàòÏßë ÏãúÏûë...")

        for page in range(1, max_pages + 1):
            try:
                # Í∞úÎÖêÍ∏Ä ÌéòÏù¥ÏßÄ URL
                url = f"{gallery['url']}?id={gallery['id']}&page={page}&exception_mode=recommend"

                html = await self.fetch_page(url)
                if not html:
                    continue

                soup = BeautifulSoup(html, 'html.parser')
                post_elements = soup.select('tr.us-post')  # Í∞úÎÖêÍ∏Ä

                page_posts = []
                for post_elem in post_elements:
                    post_data = self.parse_concept_post(post_elem, gallery)
                    if post_data:
                        page_posts.append(post_data)

                posts.extend(page_posts)
                logger.info(f"üìÑ ÌéòÏù¥ÏßÄ {page}: {len(page_posts)}Í∞ú")

                if not page_posts:  # Îçî Ïù¥ÏÉÅ Í∞úÎÖêÍ∏ÄÏù¥ ÏóÜÏúºÎ©¥ Ï§ëÎã®
                    break

            except Exception as e:
                logger.error(f"Error crawling page {page}: {str(e)}")
                continue

        logger.info(f"‚úÖ {gallery['name']}: Ï¥ù {len(posts)}Í∞ú Í∞úÎÖêÍ∏Ä ÏàòÏßë")
        return posts

    async def fetch_post_content(self, post_url: str) -> Optional[str]:
        """Í≤åÏãúÍ∏Ä Î≥∏Î¨∏ Í∞ÄÏ†∏Ïò§Í∏∞"""
        try:
            html = await self.fetch_page(post_url)
            if not html:
                return None

            soup = BeautifulSoup(html, 'html.parser')

            # Î≥∏Î¨∏ Ï∞æÍ∏∞
            content_elem = soup.select_one('.write_div') or soup.select_one('.writing_view_box')
            if content_elem:
                content = content_elem.get_text(strip=True, separator='\n')
                return content  # Ï†ÑÏ≤¥ Î≥∏Î¨∏ Ï†ÄÏû• (Îß•ÎùΩ ÌååÏïÖ ÏúÑÌï¥)

        except Exception as e:
            logger.error(f"Error fetching content from {post_url}: {str(e)}")

        return None

    async def analyze_logic_with_gpt5(self, title: str, content: str) -> Optional[Dict]:
        """GPT-5Î°ú ÎÖºÎ¶¨ Î∂ÑÏÑù"""
        if not content or len(content.strip()) < 20:
            return None

        try:
            # GPT-5 temperature Í∏∞Î≥∏Í∞í(1) ÏÇ¨Ïö©
            response = await self.openai_client.chat.completions.create(
                model=os.getenv('GPT_ANALYSIS_MODEL', 'gpt-5-mini'),
                messages=[
                    {
                        'role': 'system',
                        'content': '''ÎãπÏã†ÏùÄ ÌïúÍµ≠ Ï†ïÏπò ÎÖºÎ¶¨ Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. DCÍ∞§Îü¨Î¶¨ Í∞úÎÖêÍ∏ÄÏùÑ Î∂ÑÏÑùÌïòÏó¨ **Ïù¥ Í∏ÄÏù¥ Ïñ¥Îñ§ Ï†ïÏπòÏ†Å ÌîÑÎ†àÏûÑ/ÎÇ¥Îü¨Ìã∞Î∏åÎ•º Íµ¨ÏÑ±ÌïòÎäîÏßÄ** ÌååÏïÖÌïòÏÑ∏Ïöî.

**ÌïµÏã¨ Î™©Ï†Å**: Ïù¥ Í∏ÄÏù¥ Ïñ¥Îñ§ **ÏôúÍ≥°Îêú ÏÑ∏Í≥ÑÍ¥Ä**ÏùÑ ÎßåÎì§Ïñ¥ÎÇ¥Îäî Îç∞ Í∏∞Ïó¨ÌïòÎäîÍ∞Ä?

Ï†ïÏπòÏ†Å ÌîÑÎ†àÏûÑ/ÎÇ¥Îü¨Ìã∞Î∏å ÏòàÏãú:
- "ÎØºÏ£ºÎãπ=ÏπúÏ§ë=Íµ≠Í∞ÄÏïàÎ≥¥ÏúÑÌòë" ‚Üí Ï§ëÍµ≠Ïù∏Î¨¥ÎπÑÏûê, Ï§ëÍµ≠Ïù∏ÎèÑÎßù, Í∞ÑÏ≤©ÏùòÌòπ, ÏπúÏ§ëÏô∏Íµê Îì±Ïùò Í∏ÄÎì§Ïù¥ Ïù¥ ÌîÑÎ†àÏûÑ Íµ¨ÏÑ±
- "Ïù¥Ïû¨Î™Ö=Î≤îÏ£ÑÏûê=ÎØºÏ£ºÎãπÎ∂ïÍ¥¥" ‚Üí ÍπÄÌòúÍ≤ΩÏáºÌïë, ÎåÄÏû•ÎèôÏùòÌòπ, ÏúÑÏ¶ùÍµêÏÇ¨ Îì±Ïùò Í∏ÄÎì§Ïù¥ Ïù¥ ÌîÑÎ†àÏûÑ Íµ¨ÏÑ±
- "Ïú§ÏÑùÏó¥=Íµ≠Í∞ÄÏàòÌò∏Ïûê" ‚Üí ÏïàÎ≥¥Í∞ïÌôî, Î∂ÅÌïúÎåÄÏùë, ÎèôÎßπÍ∞ïÌôî Îì±Ïùò Í∏ÄÎì§Ïù¥ Ïù¥ ÌîÑÎ†àÏûÑ Íµ¨ÏÑ±
- "ÌïúÍµ≠Ïñ∏Î°†=Ìé∏Ìåå=Ï°∞Ïûë" ‚Üí KBSÌé∏Ìåå, MBCÏôúÍ≥°, Ïñ∏Î°†Í∞úÌòÅÌïÑÏöî Îì±Ïùò Í∏ÄÎì§Ïù¥ Ïù¥ ÌîÑÎ†àÏûÑ Íµ¨ÏÑ±

Îã§Ïùå JSON Íµ¨Ï°∞Î°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî:
{
  "core_argument": "ÌïµÏã¨ Ï£ºÏû•ÏùÑ Ìïú Î¨∏Ïû•ÏúºÎ°ú",
  "keywords": ["ÌÇ§ÏõåÎìú1", "ÌÇ§ÏõåÎìú2", "ÌÇ§ÏõåÎìú3"],
  "ai_classification": "Í≥µÍ≤©Ï†Å/Î∞©Ïñ¥Ï†Å/Ï§ëÎ¶ΩÏ†Å",
  "evidence_quality": 5,
  "threat_level": 3,
  "effectiveness_score": 7,
  "political_frame": "Ïù¥ Í∏ÄÏù¥ Íµ¨ÏÑ±ÌïòÎäî Ï†ïÏπòÏ†Å ÌîÑÎ†àÏûÑ/ÎÇ¥Îü¨Ìã∞Î∏å (Ïòà: ÎØºÏ£ºÎãπ=ÏπúÏ§ë=ÏïàÎ≥¥ÏúÑÌòë, Ïù¥Ïû¨Î™Ö=Î≤îÏ£ÑÏûê, Ïú§ÏÑùÏó¥=Íµ≠Í∞ÄÏàòÌò∏Ïûê, ÌïúÍµ≠Ïñ∏Î°†=Ìé∏Ìåå)",
  "context_issue": "Íµ¨Ï≤¥Ï†Å Ïù¥Ïäà/ÏÇ¨Í±¥ (Ïòà: Ï§ëÍµ≠Ïù∏Î¨¥ÎπÑÏûê, ÍπÄÌòúÍ≤ΩÏáºÌïë, KBSÌé∏ÌååÎ≥¥ÎèÑ)",
  "distortion_pattern": "ÏÇ¨Ïö©Îêú ÏôúÍ≥° Í∏∞Î≤ï (Ïòà: Îß•ÎùΩÏ†úÍ±∞, Í≥ºÏû•, Ïù∏Ïã†Í≥µÍ≤©, ÌóàÏúÑÏó∞Í¥Ä)"
}

**Ï§ëÏöî**:
- political_frame: Ïó¨Îü¨ Í∏ÄÎì§Ïù¥ Í≥µÏú†Ìï† Ïàò ÏûàÎäî **ÌÅ∞ ÎÇ¥Îü¨Ìã∞Î∏å/ÏÑ∏Í≥ÑÍ¥Ä**
- context_issue: Ïù¥ Í∏ÄÏù¥ Îã§Î£®Îäî **Íµ¨Ï≤¥Ï†Å ÏÇ¨Í±¥/Ïù¥Ïäà**
- Í∞ôÏùÄ political_frameÏùÑ Í∞ÄÏßÑ Í∏ÄÎì§ÏùÄ Îã§Î•∏ context_issueÎ•º Îã§Î£®ÎçîÎùºÎèÑ Í∞ôÏùÄ ÏôúÍ≥°Îêú ÏÑ∏Í≥ÑÍ¥ÄÏùÑ Íµ¨ÏÑ±Ìï®

Îã®Ïàú ÏöïÏÑ§/Ï°∞Î°±Îßå ÏûàÍ≥† Íµ¨Ï≤¥Ï†Å Ï£ºÏû•Ïù¥ ÏóÜÏúºÎ©¥ nullÏùÑ Î∞òÌôòÌïòÏÑ∏Ïöî.'''
                    },
                    {
                        'role': 'user',
                        'content': f'Ï†úÎ™©: {title}\n\nÎ≥∏Î¨∏: {content}'
                    }
                ]
                # temperature ÌååÎùºÎØ∏ÌÑ∞ Ï†úÍ±∞ (GPT-5Îäî Í∏∞Î≥∏Í∞íÎßå ÏßÄÏõê)
            )

            analysis_text = response.choices[0].message.content.strip()

            # JSON Ï∂îÏ∂ú
            if "```json" in analysis_text:
                analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
            elif analysis_text.lower() == 'null':
                return None

            try:
                return json.loads(analysis_text)
            except json.JSONDecodeError:
                logger.warning(f"JSON ÌååÏã± Ïã§Ìå®: {analysis_text[:100]}...")
                return None

        except Exception as e:
            logger.error(f"ÎÖºÎ¶¨ Î∂ÑÏÑù Ïã§Ìå® ({title[:30]}): {str(e)}")
            return None

    async def save_to_logic_repository(self, post: Dict, content: str, analysis: Dict):
        """Ïã§Ï†ú DB Ïä§ÌÇ§ÎßàÏóê ÎßûÏ∂∞ Ï†ÄÏû•"""
        try:
            # Ïã§Ï†ú logic_repository ÌÖåÏù¥Î∏î Ïª¨ÎüºÏóê ÎßûÏ∂∞ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
            logic_data = {
                'logic_type': post['logic_type'],  # attack/defense
                'source_gallery': post['gallery_id'],  # uspolitics/minjudang
                'ai_classification': analysis.get('ai_classification', 'Î∂ÑÏÑùÌïÑÏöî'),
                'core_argument': analysis.get('core_argument', ''),
                'keywords': analysis.get('keywords', []),
                'evidence_quality': analysis.get('evidence_quality', 5),
                'threat_level': analysis.get('threat_level', 3),
                'original_title': post['title'],
                'original_content': content,  # Ï†ÑÏ≤¥ Î≥∏Î¨∏ Ï†ÄÏû• (Îß•ÎùΩ ÌååÏïÖ ÏúÑÌï¥)
                'original_url': post['post_url'],
                'original_post_num': post['post_num'],
                'effectiveness_score': analysis.get('effectiveness_score', 5),
                'political_frame': analysis.get('political_frame'),  # Ï†ïÏπòÏ†Å ÌîÑÎ†àÏûÑ/ÎÇ¥Îü¨Ìã∞Î∏å
                'context_issue': analysis.get('context_issue'),  # Í¥ÄÎ†® ÏÇ¨Í±¥/Ïù¥Ïäà
                'distortion_pattern': analysis.get('distortion_pattern'),  # ÏôúÍ≥° Ìå®ÌÑ¥
                'usage_count': 0,
                'success_count': 0,
                'is_active': True,
                'created_at': datetime.now(timezone.utc).isoformat(),
                'updated_at': datetime.now(timezone.utc).isoformat()
            }

            # SupabaseÏóê Ï†ÄÏû•
            result = self.supabase.table('logic_repository').insert(logic_data).execute()

            if result.data:
                logger.info(f"‚úÖ ÎÖºÎ¶¨ Ï†ÄÏû• ÏÑ±Í≥µ: {post['title'][:30]}...")
                return result.data[0]['id']
            else:
                logger.error(f"‚ùå ÎÖºÎ¶¨ Ï†ÄÏû• Ïã§Ìå®: {post['title'][:30]}...")
                return None

        except Exception as e:
            logger.error(f"Ï†ÄÏû• Ïò§Î•ò ({post['title'][:30]}): {str(e)}")
            return None

    async def create_embedding(self, title: str, content: str, core_argument: str) -> Optional[List[float]]:
        """ÏûÑÎ≤†Îî© ÏÉùÏÑ±"""
        try:
            # Ï†úÎ™© + Î≥∏Î¨∏ + ÌïµÏã¨ÎÖºÎ¶¨Î•º Í≤∞Ìï©Ìï¥ÏÑú ÏûÑÎ≤†Îî©
            combined_text = f"{title}\n\n{content}\n\nÌïµÏã¨ÎÖºÎ¶¨: {core_argument}"

            response = await self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=combined_text[:8000]  # ÌÜ†ÌÅ∞ Ï†úÌïú
            )

            return response.data[0].embedding

        except Exception as e:
            logger.error(f"ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
            return None

    async def save_embedding(self, logic_id: str, embedding: List[float]):
        """ÏûÑÎ≤†Îî©ÏùÑ logic_repository.vector_embedding Ïª¨ÎüºÏóê Ï†ÄÏû•"""
        try:
            # logic_repository ÌÖåÏù¥Î∏îÏùò vector_embedding Ïª¨ÎüºÏóê ÏßÅÏ†ë ÏóÖÎç∞Ïù¥Ìä∏
            result = self.supabase.table('logic_repository').update({
                'vector_embedding': embedding
            }).eq('id', logic_id).execute()

            if result.data:
                logger.info(f"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: {logic_id}")
            else:
                logger.error(f"‚ùå ÏûÑÎ≤†Îî© Ï†ÄÏû• Ïã§Ìå®: {logic_id}")

        except Exception as e:
            logger.error(f"ÏûÑÎ≤†Îî© Ï†ÄÏû• Ïã§Ìå® ({logic_id}): {str(e)}")

    async def process_posts(self, posts: List[Dict], batch_size: int = 10):
        """Í≤åÏãúÍ∏ÄÎì§ Ï≤òÎ¶¨"""
        logger.info(f"üìñ {len(posts)}Í∞ú Í≤åÏãúÍ∏Ä Ï≤òÎ¶¨ ÏãúÏûë...")

        processed = 0
        for i in range(0, len(posts), batch_size):
            batch = posts[i:i+batch_size]

            for post in batch:
                try:
                    # 1. Î≥∏Î¨∏ Í∞ÄÏ†∏Ïò§Í∏∞
                    content = await self.fetch_post_content(post['post_url'])
                    if not content:
                        continue

                    # 2. ÎÖºÎ¶¨ Î∂ÑÏÑù
                    analysis = await self.analyze_logic_with_gpt5(post['title'], content)
                    if not analysis:
                        continue

                    # 3. DB Ï†ÄÏû•
                    logic_id = await self.save_to_logic_repository(post, content, analysis)
                    if not logic_id:
                        continue

                    # 4. ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ï†ÄÏû•
                    embedding = await self.create_embedding(
                        post['title'], content, analysis.get('core_argument', '')
                    )
                    if embedding:
                        await self.save_embedding(logic_id, embedding)

                    processed += 1
                    logger.info(f"‚úÖ Ï≤òÎ¶¨ ÏôÑÎ£å {processed}: {post['title'][:30]}...")

                except Exception as e:
                    logger.error(f"Í≤åÏãúÍ∏Ä Ï≤òÎ¶¨ Ïã§Ìå® ({post['title'][:30]}): {str(e)}")

            logger.info(f"üì¶ Î∞∞Ïπò {(i//batch_size)+1} ÏôÑÎ£å")

        return processed

    async def run(self):
        """Î©îÏù∏ Ïã§Ìñâ"""
        logger.info("üöÄ Fixed Logic Defense ÌÅ¨Î°§Îü¨ ÏãúÏûë!")

        total_processed = 0

        # ÎØ∏Íµ≠Ï†ïÏπò Í∞§Îü¨Î¶¨ Ï≤òÎ¶¨ (Í≥µÍ≤© ÎÖºÎ¶¨ ÏàòÏßë)
        gallery_key = 'uspolitics'
        logger.info(f"\n{'='*60}")
        logger.info(f"üéØ {GALLERIES[gallery_key]['name']} Í∞§Îü¨Î¶¨")
        logger.info(f"{'='*60}")

        try:
            # Í∞úÎÖêÍ∏Ä ÏàòÏßë
            posts = await self.fetch_concept_posts(gallery_key, max_pages=5)

            if posts:
                # Ï≤òÎ¶¨ (20Í∞ú)
                processed = await self.process_posts(posts[:20])
                total_processed += processed
                logger.info(f"üéâ {GALLERIES[gallery_key]['name']}: {processed}Í∞ú ÏôÑÎ£å")
            else:
                logger.warning(f"{GALLERIES[gallery_key]['name']}: Í∞úÎÖêÍ∏Ä ÏóÜÏùå")

        except Exception as e:
            logger.error(f"{gallery_key} Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}")

        logger.info(f"\nüéâ Ï¥ù {total_processed}Í∞ú ÎÖºÎ¶¨ Î∂ÑÏÑù ÏôÑÎ£å!")

async def main():
    if not all([SUPABASE_URL, SUPABASE_KEY, OPENAI_API_KEY]):
        logger.error("‚ùå ÌôòÍ≤ΩÎ≥ÄÏàò ÎàÑÎùΩ")
        sys.exit(1)

    async with FixedLogicCrawler() as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())