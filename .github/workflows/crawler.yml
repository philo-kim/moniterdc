name: DC Gallery Auto Crawler

on:
  schedule:
    # 5분마다 실행 (UTC 기준)
    - cron: '*/5 * * * *'
  
  workflow_dispatch:  # 수동 실행 허용
    inputs:
      gallery:
        description: 'Specific gallery to crawl (leave empty for all)'
        required: false
        default: ''

env:
  PYTHON_VERSION: '3.11'

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r crawler/requirements.txt
    
    - name: Run crawler
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        GITHUB_ACTIONS: true
      run: |
        python crawler/main.py
    
    - name: Check for high importance posts
      if: success()
      run: |
        # This step can trigger additional workflows if needed
        echo "Crawl completed successfully"
    
    - name: Error notification
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const issue = {
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Crawler failed at ${new Date().toISOString()}`,
            body: 'The automatic crawler failed. Please check the logs.',
            labels: ['bug', 'crawler']
          };
          // Uncomment to create issue on failure
          // github.rest.issues.create(issue);
