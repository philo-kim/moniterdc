#!/usr/bin/env python3
"""
Claude vs GPT ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

v2.0 íŒŒì´í”„ë¼ì¸ì„ ë‘ ëª¨ë¸ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë¹„êµ:
1. Layered Perception Extraction (3-layer)
2. Reasoning Structure Extraction (5 mechanisms)
"""

import os
import asyncio
import json
from datetime import datetime
from typing import Dict, List
from openai import AsyncOpenAI
from anthropic import Anthropic
from dotenv import load_dotenv
from engines.utils.supabase_client import get_supabase

# Load environment
load_dotenv()

# Initialize clients
openai_client = AsyncOpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
claude_client = Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY'))

# Supabase
supabase = get_supabase()


class ClaudeLayeredPerceptionExtractor:
    """Claude ë²„ì „ 3-layer perception extractor"""

    def __init__(self):
        self.client = claude_client

    async def extract(self, content: Dict) -> Dict:
        """Extract 3-layer perception using Claude"""

        prompt = f"""
ë‹¤ìŒì€ DC Gallery ì •ì¹˜ ê°¤ëŸ¬ë¦¬ì˜ ê¸€ìž…ë‹ˆë‹¤:

ì œëª©: {content['title']}
ë‚´ìš©: {content['body'][:2000]}

ì´ ê¸€ì„ **3ê°œ ì¸µìœ„**ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ ì¤‘ìš”: ì¼ë°˜ë¡ ì´ ì•„ë‹Œ, **ì´ ê¸€ì“´ì´ê°€ ì‹¤ì œë¡œ ë¯¿ëŠ” êµ¬ì²´ì ì¸ ë‚´ìš©**ì„ ì¶”ì¶œí•˜ì„¸ìš”.

## 1. í‘œë©´ì¸µ (Explicit Layer) - ëª…ì‹œì  ì£¼ìž¥
**ê¸€ì—ì„œ ì§ì ‘ ë§í•˜ê³  ìžˆëŠ” ê²ƒ**
- ëˆ„ê°€/ë¬´ì—‡ì„ ë¹„ë‚œí•˜ëŠ”ê°€?
- ì–´ë–¤ í–‰ë™/ì‚¬ê±´ì„ ë¬¸ì œ ì‚¼ëŠ”ê°€?
- êµ¬ì²´ì ì¸ ì¸ë¬¼/ì¡°ì§/ì‚¬ê±´ ì´ë¦„ í¬í•¨

## 2. ì•”ë¬µì¸µ (Implicit Layer) - ì „ì œí•˜ëŠ” ì‚¬ê³ 
**ë§í•˜ì§€ ì•Šì•˜ì§€ë§Œ ë‹¹ì—°í•˜ê²Œ ì—¬ê¸°ëŠ” ê²ƒ**

âŒ ë‚˜ìœ ì˜ˆ: "ë¹„ê³µê°œ ì •ë³´ë¥¼ ì•ˆë‹¤ = ë¶ˆë²•"
âœ… ì¢‹ì€ ì˜ˆ: "ë¯¼ì£¼ë‹¹ì€ í†µì‹ ì‚¬ë¥¼ í˜‘ë°•í•´ì„œ ê°œì¸ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤"

âŒ ë‚˜ìœ ì˜ˆ: "ì‚¬ì°°ì€ ë‚˜ì˜ë‹¤"
âœ… ì¢‹ì€ ì˜ˆ: "ì´ë“¤ì€ ë§˜ì— ì•ˆë“œëŠ” íŒì‚¬ê¹Œì§€ ì‚¬ì°°í•œë‹¤ (ì‚¬ë²•ë¶€ ìž¥ì•… ì‹œë„)"

**êµ¬ì²´ì ìœ¼ë¡œ:**
- ëˆ„ê°€ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ë¬´ì—‡ì„ í•œë‹¤ê³  ë¯¿ëŠ”ê°€?
- ê·¸ë“¤ì˜ ì˜ë„/ëª©ì ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ëŠ”ê°€?
- ì–´ë–¤ íŒ¨í„´/ì „ëžµì´ ìžˆë‹¤ê³  ë³´ëŠ”ê°€?

## 3. ì‹¬ì¸µ (Deep Layer) - ë¬´ì˜ì‹ì  ë¯¿ìŒ
**ì´ ê¸€ì“´ì´ ì§„ì˜ë§Œì˜ ì„¸ê³„ê´€**

âŒ ë‚˜ìœ ì˜ˆ: "ê¶Œë ¥ì€ ë¶€íŒ¨í•œë‹¤" (ëˆ„êµ¬ë‚˜ í•˜ëŠ” ë§)
âœ… ì¢‹ì€ ì˜ˆ: "ë¯¼ì£¼ë‹¹/ì¢ŒíŒŒëŠ” ê³¼ê±° ë…ìž¬ì •ê¶Œì²˜ëŸ¼ ì‚¬ì°°ê³¼ íƒ„ì••ìœ¼ë¡œ ê¶Œë ¥ì„ ìœ ì§€í•˜ë ¤ í•œë‹¤"

âŒ ë‚˜ìœ ì˜ˆ: "ìž‘ì€ ë¬¸ì œê°€ ì»¤ì§„ë‹¤"
âœ… ì¢‹ì€ ì˜ˆ: "ì§€ê¸ˆì˜ ìž‘ì€ ì‚¬ì°°ì´ ê³¼ê±° ë…ìž¬ì‹œëŒ€ì²˜ëŸ¼ ì „ë©´ì  ê°ì‹œêµ­ê°€ë¡œ ë°œì „í•œë‹¤"

**êµ¬ì²´ì ìœ¼ë¡œ:**
- ì´ ì§„ì˜ì´ **ì—­ì‚¬ë¥¼ ì–´ë–»ê²Œ ë³´ëŠ”ê°€**? (ê³¼ê±° ì‚¬ë¡€ â†’ í˜„ìž¬ ì—°ê²°)
- **ìƒëŒ€íŽ¸ì˜ ë³¸ì§ˆ**ì„ ì–´ë–»ê²Œ ê·œì •í•˜ëŠ”ê°€? (ë¯¼ì£¼ë‹¹/ì¢ŒíŒŒ/ì¤‘êµ­ = ?)
- **ì„¸ìƒì˜ ìž‘ë™ ì›ë¦¬**ë¥¼ ì–´ë–»ê²Œ ì´í•´í•˜ëŠ”ê°€? (Aê°€ ì¼ì–´ë‚˜ë©´ ë°˜ë“œì‹œ Bê°€ ì¼ì–´ë‚œë‹¤)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "explicit_claims": [
    {{
      "subject": "ë¯¼ì£¼ë‹¹",
      "predicate": "ìœ ì‹¬êµì²´ ì •ë³´ë¥¼ ë¶ˆë²•ìœ¼ë¡œ ì–»ì—ˆë‹¤",
      "evidence_cited": "ë‚˜ê²½ì› ì˜ì› SNS - ì–´ë–»ê²Œ ì•Œì•˜ë‚˜",
      "quote": "ìœ ì‹¬êµì²´ë¥¼ ì–´ë–»ê²Œ ì•Œì•„"
    }}
  ],
  "implicit_assumptions": [
    "ë¯¼ì£¼ë‹¹ì€ í†µì‹ ì‚¬ë¥¼ í˜‘ë°•í•´ì„œ ê°œì¸ ì‚¬ì°°ìš© ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤",
    "ë§˜ì— ì•ˆë“œëŠ” íŒì‚¬ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ì‚¬ì°°í•œë‹¤ (ì‚¬ë²•ë¶€ ìž¥ì•… ì‹œë„)"
  ],
  "reasoning_gaps": [
    {{
      "from": "ìœ ì‹¬êµì²´ ì •ë³´ë¥¼ ì•Œì•˜ë‹¤",
      "to": "í†µì‹ ì‚¬ í˜‘ë°•ìœ¼ë¡œ ì–»ì—ˆë‹¤",
      "gap": "ì •ìƒì  ë°©ë²• ê°€ëŠ¥ì„±ì€ ë°°ì œí•˜ê³  ì¦‰ì‹œ ë¶ˆë²•ìœ¼ë¡œ ë‹¨ì •"
    }}
  ],
  "deep_beliefs": [
    "ë¯¼ì£¼ë‹¹/ì¢ŒíŒŒëŠ” ê³¼ê±° ë…ìž¬ì •ê¶Œì²˜ëŸ¼ ì‚¬ì°°ë¡œ ë°˜ëŒ€íŒŒë¥¼ ì œê±°í•œë‹¤",
    "ì§€ê¸ˆì˜ ìž‘ì€ ì‚¬ì°°ì´ ê³§ ì „ë©´ì  ê°ì‹œë…ìž¬ ì‚¬íšŒë¡œ ë°œì „í•œë‹¤ (ì—­ì‚¬ ë°˜ë³µ)",
    "ì´ë“¤ì€ ì‚¬ë²•ë¶€ê¹Œì§€ ìž¥ì•…í•´ì„œ ì™„ì „í•œ ê¶Œë ¥ì„ ì°¨ì§€í•˜ë ¤ í•œë‹¤"
  ],
  "worldview_hints": "ê³¼ê±° ë…ìž¬ â†’ í˜„ìž¬ ìž¬í˜„, ì¢ŒíŒŒ = ë…ìž¬ ë³¸ì„±, ì‚¬ë²•ë¶€ ìž¥ì•… ì‹œë„"
}}
"""

        # Claude API call (sync)
        message = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4096,
            temperature=0,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        response_text = message.content[0].text

        # Parse JSON
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            json_str = response_text[json_start:json_end].strip()
        elif "{" in response_text:
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            json_str = response_text[json_start:json_end]
        else:
            json_str = response_text

        result = json.loads(json_str)
        return result


class GPTLayeredPerceptionExtractor:
    """GPT ë²„ì „ (ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜)"""

    def __init__(self):
        self.client = openai_client

    async def extract(self, content: Dict) -> Dict:
        """Extract 3-layer perception using GPT"""

        # Same prompt as Claude
        prompt = f"""
ë‹¤ìŒì€ DC Gallery ì •ì¹˜ ê°¤ëŸ¬ë¦¬ì˜ ê¸€ìž…ë‹ˆë‹¤:

ì œëª©: {content['title']}
ë‚´ìš©: {content['body'][:2000]}

ì´ ê¸€ì„ **3ê°œ ì¸µìœ„**ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

âš ï¸ ì¤‘ìš”: ì¼ë°˜ë¡ ì´ ì•„ë‹Œ, **ì´ ê¸€ì“´ì´ê°€ ì‹¤ì œë¡œ ë¯¿ëŠ” êµ¬ì²´ì ì¸ ë‚´ìš©**ì„ ì¶”ì¶œí•˜ì„¸ìš”.

## 1. í‘œë©´ì¸µ (Explicit Layer) - ëª…ì‹œì  ì£¼ìž¥
**ê¸€ì—ì„œ ì§ì ‘ ë§í•˜ê³  ìžˆëŠ” ê²ƒ**
- ëˆ„ê°€/ë¬´ì—‡ì„ ë¹„ë‚œí•˜ëŠ”ê°€?
- ì–´ë–¤ í–‰ë™/ì‚¬ê±´ì„ ë¬¸ì œ ì‚¼ëŠ”ê°€?
- êµ¬ì²´ì ì¸ ì¸ë¬¼/ì¡°ì§/ì‚¬ê±´ ì´ë¦„ í¬í•¨

## 2. ì•”ë¬µì¸µ (Implicit Layer) - ì „ì œí•˜ëŠ” ì‚¬ê³ 
**ë§í•˜ì§€ ì•Šì•˜ì§€ë§Œ ë‹¹ì—°í•˜ê²Œ ì—¬ê¸°ëŠ” ê²ƒ**

## 3. ì‹¬ì¸µ (Deep Layer) - ë¬´ì˜ì‹ì  ë¯¿ìŒ
**ì´ ê¸€ì“´ì´ ì§„ì˜ë§Œì˜ ì„¸ê³„ê´€**

JSON í˜•ì‹:
{{
  "explicit_claims": [...],
  "implicit_assumptions": [...],
  "reasoning_gaps": [...],
  "deep_beliefs": [...],
  "worldview_hints": "..."
}}
"""

        response = await self.client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "You are an expert in discourse analysis. Always respond in valid JSON format."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        return result


async def test_single_content(content_id: str = None):
    """ë‹¨ì¼ contentë¡œ GPT vs Claude ë¹„êµ"""

    print("=" * 80)
    print("Claude vs GPT ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # Get test content
    if content_id:
        query = supabase.table('contents').select('*').eq('id', content_id)
    else:
        # Get random recent content
        query = supabase.table('contents').select('*').limit(1).order('published_at', desc=True)

    result = query.execute()

    if not result.data:
        print("âŒ Content not found")
        return

    content = result.data[0]

    print(f"\nðŸ“„ í…ŒìŠ¤íŠ¸ Content:")
    print(f"   ID: {content['id']}")
    print(f"   ì œëª©: {content['title']}")
    print(f"   ë‚´ìš©: {content['body'][:200]}...")

    # Initialize extractors
    gpt_extractor = GPTLayeredPerceptionExtractor()
    claude_extractor = ClaudeLayeredPerceptionExtractor()

    # Run GPT
    print(f"\nðŸ¤– GPT-5 ë¶„ì„ ì¤‘...")
    gpt_result = await gpt_extractor.extract(content)
    print(f"   âœ“ ì™„ë£Œ")

    # Run Claude
    print(f"\nðŸ§  Claude Sonnet 4.5 ë¶„ì„ ì¤‘...")
    claude_result = await claude_extractor.extract(content)
    print(f"   âœ“ ì™„ë£Œ")

    # Compare results
    print(f"\n" + "=" * 80)
    print("ê²°ê³¼ ë¹„êµ")
    print("=" * 80)

    comparison = {
        "content": {
            "id": content['id'],
            "title": content['title'],
            "body_preview": content['body'][:500]
        },
        "gpt_result": gpt_result,
        "claude_result": claude_result,
        "comparison": {
            "explicit_claims": {
                "gpt_count": len(gpt_result.get('explicit_claims', [])),
                "claude_count": len(claude_result.get('explicit_claims', []))
            },
            "implicit_assumptions": {
                "gpt_count": len(gpt_result.get('implicit_assumptions', [])),
                "claude_count": len(claude_result.get('implicit_assumptions', []))
            },
            "deep_beliefs": {
                "gpt_count": len(gpt_result.get('deep_beliefs', [])),
                "claude_count": len(claude_result.get('deep_beliefs', []))
            }
        }
    }

    # Print comparison
    print(f"\nðŸ“Š ë ˆì´ì–´ë³„ ì¶”ì¶œ ê°œìˆ˜:")
    print(f"   Explicit Claims:  GPT {comparison['comparison']['explicit_claims']['gpt_count']:2d} | Claude {comparison['comparison']['explicit_claims']['claude_count']:2d}")
    print(f"   Implicit Assume:  GPT {comparison['comparison']['implicit_assumptions']['gpt_count']:2d} | Claude {comparison['comparison']['implicit_assumptions']['claude_count']:2d}")
    print(f"   Deep Beliefs:     GPT {comparison['comparison']['deep_beliefs']['gpt_count']:2d} | Claude {comparison['comparison']['deep_beliefs']['claude_count']:2d}")

    # Show actual content
    print(f"\nðŸ“ GPT Explicit Claims:")
    for claim in gpt_result.get('explicit_claims', [])[:3]:
        if isinstance(claim, dict):
            print(f"   - {claim.get('subject', 'N/A')}: {claim.get('predicate', 'N/A')}")
        else:
            print(f"   - {claim}")

    print(f"\nðŸ“ Claude Explicit Claims:")
    for claim in claude_result.get('explicit_claims', [])[:3]:
        if isinstance(claim, dict):
            print(f"   - {claim.get('subject', 'N/A')}: {claim.get('predicate', 'N/A')}")
        else:
            print(f"   - {claim}")

    print(f"\nðŸ’­ GPT Deep Beliefs:")
    for belief in gpt_result.get('deep_beliefs', [])[:3]:
        print(f"   - {belief}")

    print(f"\nðŸ’­ Claude Deep Beliefs:")
    for belief in claude_result.get('deep_beliefs', [])[:3]:
        print(f"   - {belief}")

    # Save results
    output_file = f"_test_results/claude_vs_gpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ ì €ìž¥: {output_file}")

    return comparison


async def main():
    """Main execution"""

    import sys

    content_id = sys.argv[1] if len(sys.argv) > 1 else None

    await test_single_content(content_id)


if __name__ == "__main__":
    asyncio.run(main())
