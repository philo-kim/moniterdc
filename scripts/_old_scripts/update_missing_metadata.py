"""
ê¸°ì¡´ contentsì˜ ëˆ„ë½ëœ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸

published_atì´ NULLì´ê±°ë‚˜ metadataì— ì¡°íšŒìˆ˜/ëŒ“ê¸€ìˆ˜ê°€ ì—†ëŠ” ê¸€ë“¤ì„
ë‹¤ì‹œ í¬ë¡¤ë§í•´ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
from engines.utils.supabase_client import get_supabase
from engines.adapters.dc_gallery_adapter import DCGalleryAdapter
from dateutil import parser as date_parser

async def update_metadata():
    supabase = get_supabase()
    adapter = DCGalleryAdapter()

    print("=" * 80)
    print("ê¸°ì¡´ contents ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸")
    print("=" * 80)
    print()

    # published_atì´ NULLì¸ ê¸€ ì°¾ê¸°
    print("ğŸ” published_atì´ NULLì¸ ê¸€ ì¡°íšŒ ì¤‘...")

    all_contents = []
    page_size = 1000
    offset = 0

    while True:
        result = supabase.table('contents')\
            .select('id, source_url, metadata')\
            .is_('published_at', 'null')\
            .range(offset, offset + page_size - 1)\
            .execute()

        if not result.data:
            break

        all_contents.extend(result.data)

        if len(result.data) < page_size:
            break

        offset += page_size

    total = len(all_contents)
    print(f"ì´ {total:,}ê°œ ê¸€ì˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ í•„ìš”")
    print()

    if total == 0:
        print("âœ… ëª¨ë“  ê¸€ì´ ì´ë¯¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        return

    updated = 0
    failed = 0

    for i, content in enumerate(all_contents, 1):
        try:
            url = content['source_url']
            content_id = content['id']

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 100 == 0 or i == 1:
                print(f"[{i:,}/{total:,}] ì²˜ë¦¬ ì¤‘... (ì„±ê³µ: {updated}, ì‹¤íŒ¨: {failed})")

            # ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§
            post_data = await adapter.fetch_post_content(url)

            if not post_data.get('body'):
                failed += 1
                continue

            # published_at íŒŒì‹±
            published_at = None
            if post_data.get('published_at'):
                try:
                    dt = date_parser.parse(post_data['published_at'])
                    published_at = dt.isoformat()
                except:
                    pass

            # metadata ì—…ë°ì´íŠ¸
            metadata = content.get('metadata', {}) or {}
            metadata.update({
                'author': post_data.get('author'),
                'view_count': post_data.get('view_count'),
                'comment_count': post_data.get('comment_count'),
                'recommend_count': post_data.get('recommend_count')
            })

            # DB ì—…ë°ì´íŠ¸
            update_data = {
                'metadata': metadata
            }

            if published_at:
                update_data['published_at'] = published_at

            supabase.table('contents')\
                .update(update_data)\
                .eq('id', content_id)\
                .execute()

            updated += 1

            # Rate limiting (ì´ˆë‹¹ 10ê°œ)
            await asyncio.sleep(0.1)

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ({url}): {e}")
            failed += 1
            continue

    print()
    print("=" * 80)
    print("ì™„ë£Œ")
    print("=" * 80)
    print(f"ì„±ê³µ: {updated:,}ê°œ")
    print(f"ì‹¤íŒ¨: {failed:,}ê°œ")
    print("=" * 80)

if __name__ == '__main__':
    asyncio.run(update_metadata())
